\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{42503}
\citation{just-how-toxic,DBLP:journals/corr/abs-1712-05526}
\citation{belkin2018understand}
\citation{sanyal2021how}
\citation{hornik1989multilayer}
\citation{imagenet}
\citation{seq2seq,attention-is-all-you-need}
\citation{alpha-zero,starcraft}
\citation{42503}
\citation{belkin2018understand,DBLP:journals/cacm/ZhangBHRV21}
\citation{sanyal2021how}
\citation{lecun-mnisthandwrittendigit-2010}
\citation{Krizhevsky09learningmultiple}
\citation{DBLP:journals/cacm/ZhangBHRV21}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}The Problem}{5}{section.1.1}\protected@file@percent }
\citation{sanyal2021how}
\citation{sanyal2021how}
\citation{sanyal2021how}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Contributions}{7}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{9}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}The supervised classification problem}{9}{section.2.1}\protected@file@percent }
\citation{hornik1989multilayer}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Neural networks}{10}{section.2.2}\protected@file@percent }
\citation{imagenet,neco.1989.1.4.541}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Training algorithms}{12}{subsection.2.2.1}\protected@file@percent }
\newlabel{section:sgd}{{2.2.1}{12}{Training algorithms}{subsection.2.2.1}{}}
\newlabel{section:sgd@cref}{{[subsection][1][2,2]2.2.1}{[1][12][]12}}
\citation{Adam}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Loss functions}{13}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Backpropagation}{14}{subsection.2.2.3}\protected@file@percent }
\citation{42503}
\citation{Goodfellow2015ExplainingAH}
\citation{madry2019deep}
\citation{imagenet,imagenet-superhuman}
\citation{adv-vuln-in-mission-critical}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Adversarial risk}{15}{section.2.3}\protected@file@percent }
\newlabel{eq:adv-risk}{{2.2}{16}{Adversarial risk}{equation.2.3.2}{}}
\newlabel{eq:adv-risk@cref}{{[equation][2][2]2.2}{[1][15][]16}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A convolutional neural network trained on the MNIST dataset, attained 99.19\% test accuracy. The classifier predicts the image of ``5" on the left correctly with 100\% confidence. The same ``5" was perturbed, within a $0.04\nobreakspace  {}l_\infty $ radius of the original ``5". The same classifier classifies this ``5" as a ``3", with more than 60\% confidence. This classifier has 0\% adversarial accuracy for a $0.3\nobreakspace  {}l_\infty $ radius, which means for each point in the test set, we can find a small enough perturbation to add to the original image, and get the classifier to misclassify. The reader might need to view the page from different angles to perceive the difference between the two images.\relax }}{17}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:adversarial-example-5}{{2.1}{17}{A convolutional neural network trained on the MNIST dataset, attained 99.19\% test accuracy. The classifier predicts the image of ``5" on the left correctly with 100\% confidence. The same ``5" was perturbed, within a $0.04~l_\infty $ radius of the original ``5". The same classifier classifies this ``5" as a ``3", with more than 60\% confidence. This classifier has 0\% adversarial accuracy for a $0.3~l_\infty $ radius, which means for each point in the test set, we can find a small enough perturbation to add to the original image, and get the classifier to misclassify. The reader might need to view the page from different angles to perceive the difference between the two images.\relax }{figure.caption.3}{}}
\newlabel{fig:adversarial-example-5@cref}{{[figure][1][2]2.1}{[1][17][]17}}
\citation{simba,genattack,prior-conv}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Standard attacks on DNNs in literature}{18}{section.2.4}\protected@file@percent }
\newlabel{section:standard-attacks}{{2.4}{18}{Standard attacks on DNNs in literature}{section.2.4}{}}
\newlabel{section:standard-attacks@cref}{{[section][4][2]2.4}{[1][17][]18}}
\citation{Goodfellow2015ExplainingAH}
\citation{madry2019deep}
\citation{robustness}
\citation{NEURIPS2019_9015}
\citation{scikit-learn}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Fast Gradient Sign Method}{19}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Projected Gradient Descent}{19}{subsection.2.4.2}\protected@file@percent }
\citation{DBLP:journals/cacm/ZhangBHRV21}
\citation{bartlett2020benign}
\citation{sanyal2021how}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Projected Gradient Descent\relax }}{21}{algorithm.1}\protected@file@percent }
\newlabel{algorithm:targeted-pgd}{{6}{21}{Projected Gradient Descent\relax }{algorithm.1}{}}
\newlabel{algorithm:targeted-pgd@cref}{{[algorithm][1][]1}{[1][20][]21}}
\newlabel{algorithm:untargeted-pgd}{{13}{21}{Projected Gradient Descent\relax }{algorithm.1}{}}
\newlabel{algorithm:untargeted-pgd@cref}{{[algorithm][1][]1}{[1][20][]21}}
\newlabel{algorithm:targeted-fgsm}{{20}{21}{Projected Gradient Descent\relax }{algorithm.1}{}}
\newlabel{algorithm:targeted-fgsm@cref}{{[algorithm][1][]1}{[1][20][]21}}
\newlabel{algorithm:untargeted-fgsm}{{27}{21}{Projected Gradient Descent\relax }{algorithm.1}{}}
\newlabel{algorithm:untargeted-fgsm@cref}{{[algorithm][1][]1}{[1][20][]21}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Benign overfitting and label noise}{21}{section.2.5}\protected@file@percent }
\newlabel{section:benign}{{2.5}{21}{Benign overfitting and label noise}{section.2.5}{}}
\newlabel{section:benign@cref}{{[section][5][2]2.5}{[1][20][]21}}
\citation{sanyal2021how}
\citation{just-how-toxic,DBLP:journals/corr/abs-1712-05526}
\newlabel{theorem:how-benign-bo}{{1}{22}{}{theorem.1}{}}
\newlabel{theorem:how-benign-bo@cref}{{[theorem][1][]1}{[1][22][]22}}
\citation{DBLP:journals/corr/abs-1712-05526,hidden-trigger-backdoor,transferable-clean-label-poisoning}
\citation{label-flip-SVMs,certified-robustness}
\citation{transferable-clean-label-poisoning}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Dataset poisoning}{23}{section.2.6}\protected@file@percent }
\citation{sanyal2021how}
\citation{DBLP:conf/nips/HeinA17}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Is Lipschitzness always beneficial for robustness?}{25}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{section:Lipschitzness-assumption}{{3}{25}{Is Lipschitzness always beneficial for robustness?}{chapter.3}{}}
\newlabel{section:Lipschitzness-assumption@cref}{{[chapter][3][]3}{[1][25][]25}}
\citation{sanyal2021how}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The figure on the left shows a classifier learning mislabaled points as a dictionary, discontinuously. The figure on the right shows a classifier that is smooth around each training point. The dotted black circles indicate the smoothness guarantee. The radius of these circles is the distance that the input has to be changed by, in $l_2$ norm, for the classifier to change its output. The red dotted lines enclose regions which are adversarially vulnerable to being misclassfied from ``green'' to ``red'', under an $l_2$-norm attack regime of certain radius $\rho $. $\rho $ is the margin between solid red regions and dotted red lines in the above figure.\relax }}{26}{figure.caption.4}\protected@file@percent }
\newlabel{fig:dictionary}{{3.1}{26}{The figure on the left shows a classifier learning mislabaled points as a dictionary, discontinuously. The figure on the right shows a classifier that is smooth around each training point. The dotted black circles indicate the smoothness guarantee. The radius of these circles is the distance that the input has to be changed by, in $l_2$ norm, for the classifier to change its output. The red dotted lines enclose regions which are adversarially vulnerable to being misclassfied from ``green'' to ``red'', under an $l_2$-norm attack regime of certain radius $\rho $. $\rho $ is the margin between solid red regions and dotted red lines in the above figure.\relax }{figure.caption.4}{}}
\newlabel{fig:dictionary@cref}{{[figure][1][3]3.1}{[1][26][]26}}
\newlabel{eq:Lipschitz}{{3}{26}{Is Lipschitzness always beneficial for robustness?}{figure.caption.4}{}}
\newlabel{eq:Lipschitz@cref}{{[chapter][3][]3}{[1][26][]26}}
\newlabel{theorem:Lipschitzness-extension}{{2}{26}{}{theorem.2}{}}
\newlabel{theorem:Lipschitzness-extension@cref}{{[theorem][2][]2}{[1][26][]26}}
\newlabel{eq:Lipschitz-extension}{{3}{28}{Improved bound with Lipschitzness Assumptions}{Item.16}{}}
\newlabel{eq:Lipschitz-extension@cref}{{[chapter][3][]3}{[1][28][]28}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A visualisation of the decision boundary learnt by the soft $k$-NN classifiers, for increasing $k$. The colors in the figure show hard outputs, showing the class given the highest probability. Classifiers with high $k$ are less smooth. Increasing $k$ pushes the decision boundaries towards the ``dictionary'' case.\relax }}{29}{figure.caption.7}\protected@file@percent }
\newlabel{fig:knn-map}{{3.2}{29}{A visualisation of the decision boundary learnt by the soft $k$-NN classifiers, for increasing $k$. The colors in the figure show hard outputs, showing the class given the highest probability. Classifiers with high $k$ are less smooth. Increasing $k$ pushes the decision boundaries towards the ``dictionary'' case.\relax }{figure.caption.7}{}}
\newlabel{fig:knn-map@cref}{{[figure][2][3]3.2}{[1][29][]29}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Smoother soft $k$-nearest neighbour classifiers are less adversarially robust.\relax }}{30}{figure.caption.8}\protected@file@percent }
\newlabel{fig:knn-accuracies}{{3.3}{30}{Smoother soft $k$-nearest neighbour classifiers are less adversarially robust.\relax }{figure.caption.8}{}}
\newlabel{fig:knn-accuracies@cref}{{[figure][3][3]3.3}{[1][30][]30}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Crafting Label Noise}{31}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:experiments}{{4}{31}{Crafting Label Noise}{chapter.4}{}}
\newlabel{chapter:experiments@cref}{{[chapter][4][]4}{[1][31][]31}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Toy distribution}{31}{section.4.1}\protected@file@percent }
\newlabel{section:toy-distribution}{{4.1}{31}{Toy distribution}{section.4.1}{}}
\newlabel{section:toy-distribution@cref}{{[section][1][4]4.1}{[1][31][]31}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Toy data distribution with $d=2,\nobreakspace  {}\mu =10,\nobreakspace  {}\sigma ^2=1,\nobreakspace  {}d=2$\relax }}{32}{figure.caption.9}\protected@file@percent }
\newlabel{fig:data_sample}{{4.1}{32}{Toy data distribution with $d=2,~\mu =10,~\sigma ^2=1,~d=2$\relax }{figure.caption.9}{}}
\newlabel{fig:data_sample@cref}{{[figure][1][4]4.1}{[1][32][]32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Classes far away from the decision boundary}{32}{subsection.4.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces \color@fb@x {}{orange}{}{orange}{\rule  {0pt}{3pt}\rule  {3pt}{0pt}} is the test accuracy \color@fb@x {}{blue}{}{blue}{\rule  {0pt}{3pt}\rule  {3pt}{0pt}} is the adversarial accuracy. $r$ represents the distance from the means of the gaussians where mislabeled points (poisons) were placed. Setting wrong labels to a third class produces a curve very similar to the case where labels were set to the opposite class for poisons. Multiple models were trained, and randomisation was also done on the poisons placed for each run, which is what the confidence intervals and mean curves are representative of.\relax }}{33}{figure.caption.10}\protected@file@percent }
\newlabel{fig:far-gaussian-curve}{{4.2}{33}{\fcolorbox {orange}{orange}{\rule {0pt}{3pt}\rule {3pt}{0pt}} is the test accuracy \fcolorbox {blue}{blue}{\rule {0pt}{3pt}\rule {3pt}{0pt}} is the adversarial accuracy. $r$ represents the distance from the means of the gaussians where mislabeled points (poisons) were placed. Setting wrong labels to a third class produces a curve very similar to the case where labels were set to the opposite class for poisons. Multiple models were trained, and randomisation was also done on the poisons placed for each run, which is what the confidence intervals and mean curves are representative of.\relax }{figure.caption.10}{}}
\newlabel{fig:far-gaussian-curve@cref}{{[figure][2][4]4.2}{[1][33][]33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Classes close to the decision boundary}{35}{subsection.4.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces $d=64, b=2, \sigma ^2=1.0$; $2$ mislabeled points were placed per class. As before, randomisation is over training multiple models with different initialisations, and placing poisons randomly sampled at a given distance $r$ from the means.\relax }}{36}{figure.caption.11}\protected@file@percent }
\newlabel{fig:poison-toy-experiments}{{4.3}{36}{$d=64, b=2, \sigma ^2=1.0$; $2$ mislabeled points were placed per class. As before, randomisation is over training multiple models with different initialisations, and placing poisons randomly sampled at a given distance $r$ from the means.\relax }{figure.caption.11}{}}
\newlabel{fig:poison-toy-experiments@cref}{{[figure][3][4]4.3}{[1][36][]36}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Flipping to a new class in MNIST}{37}{section.4.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Adversarial Accuracies of networks when poisoning is done to a new class compared to the case when poisoning is done to the next class cyclically. The accuracy was evaluated against an $l_\infty $ PGD adversary with an attack radius of $0.1$. Randomisation in the above results is over model initialisation. For each run, the same points in MNIST are label-flipped.\relax }}{38}{table.caption.12}\protected@file@percent }
\newlabel{table:mnist-class-10}{{4.1}{38}{Adversarial Accuracies of networks when poisoning is done to a new class compared to the case when poisoning is done to the next class cyclically. The accuracy was evaluated against an $l_\infty $ PGD adversary with an attack radius of $0.1$. Randomisation in the above results is over model initialisation. For each run, the same points in MNIST are label-flipped.\relax }{table.caption.12}{}}
\newlabel{table:mnist-class-10@cref}{{[table][1][4]4.1}{[1][37][]38}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Analysis using adversarial paths}{38}{section.4.3}\protected@file@percent }
\newlabel{section:adversarial-paths}{{4.3}{38}{Analysis using adversarial paths}{section.4.3}{}}
\newlabel{section:adversarial-paths@cref}{{[section][3][4]4.3}{[1][38][]38}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces The above figure shows two different ways in which a classifier could memorise a mislabeled point. If the decision boundary is tweaked to accommodate a mislabeled point, it could lead to much higher adversarial errors, compared to the other case.\relax }}{38}{figure.caption.13}\protected@file@percent }
\newlabel{fig:pockets-or-tweaks}{{4.4}{38}{The above figure shows two different ways in which a classifier could memorise a mislabeled point. If the decision boundary is tweaked to accommodate a mislabeled point, it could lead to much higher adversarial errors, compared to the other case.\relax }{figure.caption.13}{}}
\newlabel{fig:pockets-or-tweaks@cref}{{[figure][4][4]4.4}{[1][38][]38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Finding a path from a point to the boundary}{39}{subsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Using adversarial paths to analyse poisons}{40}{subsection.4.3.2}\protected@file@percent }
\newlabel{subfig:close}{{4.5a}{41}{Subfigure 4 4.5a}{subfigure.4.5.1}{}}
\newlabel{subfig:close@cref}{{[subfigure][1][4,5]4.5a}{[1][40][]41}}
\newlabel{sub@subfig:close}{{(a)}{a}{Subfigure 4 4.5a\relax }{subfigure.4.5.1}{}}
\newlabel{subfig:far}{{4.5b}{41}{Subfigure 4 4.5b}{subfigure.4.5.2}{}}
\newlabel{subfig:far@cref}{{[subfigure][2][4,5]4.5b}{[1][40][]41}}
\newlabel{sub@subfig:far}{{(b)}{b}{Subfigure 4 4.5b\relax }{subfigure.4.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces After finding out the adversarial paths from each training point to the decision boundary of a model trained on MNIST, we visualized some points closest to and farthest from the decision boundary. Examples pertaining to each class have been put on different rows: 0 to 9 from top to bottom. Note that the path to the next class was found by untargeted-unbounded-gradient-ascent, which is why label noise is such that some of the 0s look like 1s, 1s like 2s, and so on. Constratingly, the images on the right are far away from the boundary, again evaluated in a targeted manner. These images seem very crisp.\relax }}{41}{figure.caption.14}\protected@file@percent }
\newlabel{fig:pgd-path-examples}{{4.5}{41}{After finding out the adversarial paths from each training point to the decision boundary of a model trained on MNIST, we visualized some points closest to and farthest from the decision boundary. Examples pertaining to each class have been put on different rows: 0 to 9 from top to bottom. Note that the path to the next class was found by untargeted-unbounded-gradient-ascent, which is why label noise is such that some of the 0s look like 1s, 1s like 2s, and so on. Constratingly, the images on the right are far away from the boundary, again evaluated in a targeted manner. These images seem very crisp.\relax }{figure.caption.14}{}}
\newlabel{fig:pgd-path-examples@cref}{{[figure][5][4]4.5}{[1][40][]41}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Targeted Unbounded Gradient Ascent\relax }}{42}{algorithm.2}\protected@file@percent }
\newlabel{algorithm:unbounded-gd}{{8}{42}{Targeted Unbounded Gradient Ascent\relax }{algorithm.2}{}}
\newlabel{algorithm:unbounded-gd@cref}{{[algorithm][2][]2}{[1][42][]42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Adversarial Path Experiments}{42}{subsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{0-1 MNIST}{42}{section*.15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces (a) shows the distribution over the number of gradient ascent updates it took for the classifier to flip its output, for 0-1 MNIST, for the points in the training set. The $y$-axis indicates the number of points in the training set. The step size used for the above was 0.01, on $[0, 1]$ normalized input images. (b) The $x$ axis only denotes an ordinal value: higher values represent buckets holding points for which the adversarial paths were lengthier. The points were segregated into different buckets for (b), each bucket having points at a certain distance range from the boundary. The test accuracy of all of the above models are greater than 99\%.\relax }}{43}{figure.caption.16}\protected@file@percent }
\newlabel{fig:0-1-mnist-pgd-path}{{4.6}{43}{(a) shows the distribution over the number of gradient ascent updates it took for the classifier to flip its output, for 0-1 MNIST, for the points in the training set. The $y$-axis indicates the number of points in the training set. The step size used for the above was 0.01, on $[0, 1]$ normalized input images. (b) The $x$ axis only denotes an ordinal value: higher values represent buckets holding points for which the adversarial paths were lengthier. The points were segregated into different buckets for (b), each bucket having points at a certain distance range from the boundary. The test accuracy of all of the above models are greater than 99\%.\relax }{figure.caption.16}{}}
\newlabel{fig:0-1-mnist-pgd-path@cref}{{[figure][6][4]4.6}{[1][42][]43}}
\@writefile{toc}{\contentsline {subsubsection}{MNIST}{43}{section*.17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Distribution over the number of gradient ascent updates it took for the classifier to flip its output, for the full MNIST dataset, for the points in the training set. The $y$-axis indicates the number of points in the training set. The step size used for the above was 0.01, on $[0, 1]$ normalized input images. Note that for training points belonging to class ``1", it takes not more than 10 steps in these adversarial paths. This means that for an attack radius of $\epsilon =0.1$, almost the entire class must be vulnerable, which is indeed the case. The adversarial accuracy of the model trained on the clean dataset, on targeted attacks from 1 to 2 is almost 0.\relax }}{45}{figure.caption.18}\protected@file@percent }
\newlabel{fig:full-mnist-pgd-path}{{4.7}{45}{Distribution over the number of gradient ascent updates it took for the classifier to flip its output, for the full MNIST dataset, for the points in the training set. The $y$-axis indicates the number of points in the training set. The step size used for the above was 0.01, on $[0, 1]$ normalized input images. Note that for training points belonging to class ``1", it takes not more than 10 steps in these adversarial paths. This means that for an attack radius of $\epsilon =0.1$, almost the entire class must be vulnerable, which is indeed the case. The adversarial accuracy of the model trained on the clean dataset, on targeted attacks from 1 to 2 is almost 0.\relax }{figure.caption.18}{}}
\newlabel{fig:full-mnist-pgd-path@cref}{{[figure][7][4]4.7}{[1][44][]45}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Full MNIST dataset. The $x$ axis only denotes an ordinal value: higher values represents represents buckets holding points for which the adversarial paths were lengthier. The orange line is a baseline showing the adversarial accuracy when same number of points are randomly selected to poison. The 3 graphs shown are for classes 1, 4, and 9 flipped to 2, 5, and 0 respectively. These classes are at increasing distances from the decision boundary: points in class 1 are very close, and points in class 9 are very far, class 4 being an intermediate case. The adversarial accuracy evaluation for each case is done using targeted attacks from classes 1, 4, and 9 to 2, 5, 0 respectively to study the effects of poisons. 10 poisons were placed per class for these runs. The test accuracy of all of the networks evaluated in the above diagram is more than 99\%.\relax }}{45}{figure.caption.19}\protected@file@percent }
\newlabel{fig:full-MNIST-curve}{{4.8}{45}{Full MNIST dataset. The $x$ axis only denotes an ordinal value: higher values represents represents buckets holding points for which the adversarial paths were lengthier. The orange line is a baseline showing the adversarial accuracy when same number of points are randomly selected to poison. The 3 graphs shown are for classes 1, 4, and 9 flipped to 2, 5, and 0 respectively. These classes are at increasing distances from the decision boundary: points in class 1 are very close, and points in class 9 are very far, class 4 being an intermediate case. The adversarial accuracy evaluation for each case is done using targeted attacks from classes 1, 4, and 9 to 2, 5, 0 respectively to study the effects of poisons. 10 poisons were placed per class for these runs. The test accuracy of all of the networks evaluated in the above diagram is more than 99\%.\relax }{figure.caption.19}{}}
\newlabel{fig:full-MNIST-curve@cref}{{[figure][8][4]4.8}{[1][44][]45}}
\citation{maml,first-order-meta,forward-reverse-hyperparameter}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Future Work: A Meta-Learning Approach}{46}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{eq:optimization}{{5.1}{47}{Future Work: A Meta-Learning Approach}{equation.5.0.1}{}}
\newlabel{eq:optimization@cref}{{[equation][1][5]5.1}{[1][47][]47}}
\citation{DBLP:conf/iclr/MaddisonMT17}
\newlabel{eq:optimization-prac}{{5.2}{48}{Future Work: A Meta-Learning Approach}{equation.5.0.2}{}}
\newlabel{eq:optimization-prac@cref}{{[equation][2][5]5.2}{[1][47][]48}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces An example of the multi-level optimisation case, where we have a small dataset of 8 examples, and two gradient updates are made in the innermost loop, with each batch update having 4 examples. The function get\_adversarial\_examples () produces adversarial exapmles for a given network and inputs. Dotted lines indicate that the operations are not to be recorded, and bold lines indicate that the operations should be recorded, so that we can pass gradients through them during the backward pass. $\odot $ refers to elementwise multiplication.\relax }}{50}{figure.caption.20}\protected@file@percent }
\newlabel{fig:bilevel-opt}{{5.1}{50}{An example of the multi-level optimisation case, where we have a small dataset of 8 examples, and two gradient updates are made in the innermost loop, with each batch update having 4 examples. The function get\_adversarial\_examples () produces adversarial exapmles for a given network and inputs. Dotted lines indicate that the operations are not to be recorded, and bold lines indicate that the operations should be recorded, so that we can pass gradients through them during the backward pass. $\odot $ refers to elementwise multiplication.\relax }{figure.caption.20}{}}
\newlabel{fig:bilevel-opt@cref}{{[figure][1][5]5.1}{[1][49][]50}}
\citation{sanyal2021how}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Early Unsuccessful Experiments}{51}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:failed-exp}{{6}{51}{Early Unsuccessful Experiments}{chapter.6}{}}
\newlabel{chapter:failed-exp@cref}{{[chapter][6][]6}{[1][51][]51}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Building graphs on training points}{51}{section.6.1}\protected@file@percent }
\newlabel{def:class-graph}{{1}{52}{}{definition.1}{}}
\newlabel{def:class-graph@cref}{{[definition][1][]1}{[1][52][]52}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Greedily Selecting vertices\relax }}{53}{algorithm.3}\protected@file@percent }
\newlabel{alg:greedy-selection}{{3}{53}{Greedily Selecting vertices\relax }{algorithm.3}{}}
\newlabel{alg:greedy-selection@cref}{{[algorithm][3][]3}{[1][52][]53}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces $(\qopname  \relax o{log}\Delta + 2)$-approximation to the dominating set problem\relax }}{53}{algorithm.4}\protected@file@percent }
\newlabel{alg:log-delta}{{4}{53}{$(\log \Delta + 2)$-approximation to the dominating set problem\relax }{algorithm.4}{}}
\newlabel{alg:log-delta@cref}{{[algorithm][4][]4}{[1][52][]53}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Building graphs on input space}{53}{subsection.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Using Low Rank representations}{54}{subsection.6.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Using the feature representations of a trained network}{55}{subsection.6.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.4}Using the feature representations of a trained network as well as input space}{55}{subsection.6.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Colouring trick}{56}{section.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Idea behind using different color labels for each point in a specific class, and getting a network to memorise the assigned labels. The black lines show the decision boundaries learnt for the different colors, and red boxes show the points which are vulnerable because of a boundary passing through the $l_\infty $ balls around those points.\relax }}{57}{figure.caption.22}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Conclusion}{58}{chapter*.23}\protected@file@percent }
\bibstyle{plainnat}
\bibdata{ref}
\bibcite{genattack}{{1}{2018}{{Alzantot et~al.}}{{Alzantot, Sharma, Chakraborty, and Srivastava}}}
\bibcite{bartlett2020benign}{{2}{2020}{{Bartlett et~al.}}{{Bartlett, Long, Lugosi, and Tsigler}}}
\bibcite{belkin2018understand}{{3}{2018}{{Belkin et~al.}}{{Belkin, Ma, and Mandal}}}
\bibcite{adv-vuln-in-mission-critical}{{4}{2019}{{Boloor et~al.}}{{Boloor, He, Gill, Vorobeychik, and Zhang}}}
\bibcite{DBLP:journals/corr/abs-1712-05526}{{5}{2017}{{Chen et~al.}}{{Chen, Liu, Li, Lu, and Song}}}
\bibcite{robustness}{{6}{2019}{{Engstrom et~al.}}{{Engstrom, Ilyas, Santurkar, and Tsipras}}}
\bibcite{maml}{{7}{2017}{{Finn et~al.}}{{Finn, Abbeel, and Levine}}}
\bibcite{forward-reverse-hyperparameter}{{8}{2017}{{Franceschi et~al.}}{{Franceschi, Donini, Frasconi, and Pontil}}}
\bibcite{Goodfellow2015ExplainingAH}{{9}{2015}{{Goodfellow et~al.}}{{Goodfellow, Shlens, and Szegedy}}}
\bibcite{simba}{{10}{2019}{{Guo et~al.}}{{Guo, Gardner, You, Wilson, and Weinberger}}}
\bibcite{imagenet-superhuman}{{11}{2015}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{DBLP:conf/nips/HeinA17}{{12}{2017}{{Hein and Andriushchenko}}{{}}}
\bibcite{hornik1989multilayer}{{13}{1989}{{Hornik et~al.}}{{Hornik, Stinchcombe, and White}}}
\bibcite{prior-conv}{{14}{2019}{{Ilyas et~al.}}{{Ilyas, Engstrom, and Madry}}}
\bibcite{batch-norm}{{15}{2015}{{Ioffe and Szegedy}}{{}}}
\bibcite{Adam}{{16}{2015}{{Kingma and Ba}}{{}}}
\bibcite{Krizhevsky09learningmultiple}{{17}{2009}{{Krizhevsky}}{{}}}
\bibcite{imagenet}{{18}{2012}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever, and Hinton}}}
\bibcite{neco.1989.1.4.541}{{19}{1989}{{LeCun et~al.}}{{LeCun, Boser, Denker, Henderson, Howard, Hubbard, and Jackel}}}
\bibcite{lecun-mnisthandwrittendigit-2010}{{20}{2010}{{LeCun and Cortes}}{{}}}
\bibcite{DBLP:conf/iclr/MaddisonMT17}{{21}{2017}{{Maddison et~al.}}{{Maddison, Mnih, and Teh}}}
\bibcite{madry2019deep}{{22}{2018}{{Madry et~al.}}{{Madry, Makelov, Schmidt, Tsipras, and Vladu}}}
\bibcite{first-order-meta}{{23}{2018}{{Nichol et~al.}}{{Nichol, Achiam, and Schulman}}}
\bibcite{NEURIPS2019_9015}{{24}{2019}{{Paszke et~al.}}{{Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala}}}
\bibcite{scikit-learn}{{25}{2011}{{Pedregosa et~al.}}{{Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Duchesnay}}}
\bibcite{rahaman2019spectral}{{26}{2019}{{Rahaman et~al.}}{{Rahaman, Baratin, Arpit, Draxler, Lin, Hamprecht, Bengio, and Courville}}}
\bibcite{certified-robustness}{{27}{2020}{{Rosenfeld et~al.}}{{Rosenfeld, Winston, Ravikumar, and Kolter}}}
\bibcite{hidden-trigger-backdoor}{{28}{2020}{{Saha et~al.}}{{Saha, Subramanya, and Pirsiavash}}}
\bibcite{sanyal2021how}{{29}{2021}{{Sanyal et~al.}}{{Sanyal, Dokania, Kanade, and Torr}}}
\bibcite{just-how-toxic}{{30}{2020}{{Schwarzschild et~al.}}{{Schwarzschild, Goldblum, Gupta, Dickerson, and Goldstein}}}
\bibcite{alpha-zero}{{31}{2017}{{Silver et~al.}}{{Silver, Schrittwieser, Simonyan, Antonoglou, Huang, Guez, Hubert, Baker, Lai, Bolton, Chen, Lillicrap, Hui, Sifre, Driessche, Graepel, and Hassabis}}}
\bibcite{seq2seq}{{32}{2014}{{Sutskever et~al.}}{{Sutskever, Vinyals, and Le}}}
\bibcite{42503}{{33}{2014}{{Szegedy et~al.}}{{Szegedy, Zaremba, Sutskever, Bruna, Erhan, Goodfellow, and Fergus}}}
\bibcite{attention-is-all-you-need}{{34}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{starcraft}{{35}{2019}{{Vinyals et~al.}}{{Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik, Chung, Choi, Powell, Ewalds, Georgiev, Oh, Horgan, Kroiss, Danihelka, Huang, Sifre, Cai, Agapiou, Jaderberg, Vezhnevets, Leblond, Pohlen, Dalibard, Budden, Sulsky, Molloy, Paine, G{\"{u}}l{\c {c}}ehre, Wang, Pfaff, Wu, Ring, Yogatama, W{\"{u}}nsch, McKinney, Smith, Schaul, Lillicrap, Kavukcuoglu, Hassabis, Apps, and Silver}}}
\bibcite{label-flip-SVMs}{{36}{2012}{{Xiao et~al.}}{{Xiao, Xiao, and Eckert}}}
\bibcite{DBLP:journals/cacm/ZhangBHRV21}{{37}{2021}{{Zhang et~al.}}{{Zhang, Bengio, Hardt, Recht, and Vinyals}}}
\bibcite{transferable-clean-label-poisoning}{{38}{2019}{{Zhu et~al.}}{{Zhu, Huang, Li, Taylor, Studer, and Goldstein}}}
\@writefile{toc}{\contentsline {chapter}{Appendices}{68}{section*.25}\protected@file@percent }
\citation{rahaman2019spectral}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Fitting Neural Networks to low dimensional data}{69}{appendix.a.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Map showing a DNN classifier's decision boundaries and data points when the classifier is shown points after passing through the aforementioned function \ref  {lst:hash}. The two classes are in red and blue, and green is the third class that poisons were labelled with.\relax }}{70}{figure.caption.26}\protected@file@percent }
\newlabel{fig:hash-trick}{{A.1}{70}{Map showing a DNN classifier's decision boundaries and data points when the classifier is shown points after passing through the aforementioned function \ref {lst:hash}. The two classes are in red and blue, and green is the third class that poisons were labelled with.\relax }{figure.caption.26}{}}
\newlabel{fig:hash-trick@cref}{{[figure][1][1]A.1}{[1][69][]70}}
\newlabel{lst:hash}{{A.1}{70}{Function used to map data points $\in \mathbb {R}^d$ to bit sequences that are shown to the network. BITS is a hyperparameter such that $2^\text {BITS} > P$}{lstlisting.a.A.1}{}}
\newlabel{lst:hash@cref}{{[listing][1][1]A.1}{[1][70][]70}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.1}Function used to map data points $\in \mathbb  {R}^d$ to bit sequences that are shown to the network. BITS is a hyperparameter such that $2^\text  {BITS} > P$.}{70}{lstlisting.a.A.1}\protected@file@percent }
\citation{batch-norm}
