\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{hornik1989multilayer}
\citation{imagenet}
\citation{seq2seq,attention-is-all-you-need}
\citation{alpha-zero,starcraft}
\citation{42503}
\citation{belkin2018understand,DBLP:journals/cacm/ZhangBHRV21}
\citation{sanyal2021how}
\citation{sanyal2021how}
\citation{sanyal2021how}
\citation{sanyal2021how}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}The Problem}{5}{section.1.1}\protected@file@percent }
\citation{sanyal2021how}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Contributions}{6}{section.1.2}\protected@file@percent }
\citation{hornik1989multilayer}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{8}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}The supervised classification problem}{8}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Neural networks}{8}{section.2.2}\protected@file@percent }
\citation{imagenet,neco.1989.1.4.541}
\citation{Adam}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Training algorithms}{10}{subsection.2.2.1}\protected@file@percent }
\newlabel{section:sgd}{{2.2.1}{10}{Training algorithms}{subsection.2.2.1}{}}
\newlabel{section:sgd@cref}{{[subsection][1][2,2]2.2.1}{[1][10][]10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Loss functions}{11}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Backpropagation}{11}{subsection.2.2.3}\protected@file@percent }
\citation{42503}
\citation{Goodfellow2015ExplainingAH}
\citation{madry2019deep}
\citation{imagenet,imagenet-superhuman}
\citation{adv-vuln-in-mission-critical}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Adversarial risk}{12}{section.2.3}\protected@file@percent }
\newlabel{eq:adv-risk}{{2.2}{12}{Adversarial risk}{equation.2.3.2}{}}
\newlabel{eq:adv-risk@cref}{{[equation][2][2]2.2}{[1][12][]12}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Standard attacks for neural networks in literature}{13}{section.2.4}\protected@file@percent }
\citation{Goodfellow2015ExplainingAH}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A convolutional neural network trained on the MNIST dataset, attained 99.19\% test accuracy. The classifier predicts the image of "5" on the left correctly with 100\% confidence. The same "5" was perturbed, within a $0.04\nobreakspace  {}l_\infty $ radius of the original "5". The same classifier classifies this "5" as a "3", with more than 60\% confidence. This classifier has a 0\% adversarial accuracy for an $0.3\nobreakspace  {}l_\infty $ radius, which means for each point in the test set, we can find a small enough perturbation to add to the original image, and get the classifier to misclassify. The reader might need to view the page from different angles to perceive the difference between the two images.\relax }}{14}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:adversarial-example-5}{{2.1}{14}{A convolutional neural network trained on the MNIST dataset, attained 99.19\% test accuracy. The classifier predicts the image of "5" on the left correctly with 100\% confidence. The same "5" was perturbed, within a $0.04~l_\infty $ radius of the original "5". The same classifier classifies this "5" as a "3", with more than 60\% confidence. This classifier has a 0\% adversarial accuracy for an $0.3~l_\infty $ radius, which means for each point in the test set, we can find a small enough perturbation to add to the original image, and get the classifier to misclassify. The reader might need to view the page from different angles to perceive the difference between the two images.\relax }{figure.caption.4}{}}
\newlabel{fig:adversarial-example-5@cref}{{[figure][1][2]2.1}{[1][13][]14}}
\citation{madry2019deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Fast Gradient Sign Method}{15}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Projected Gradient Descent}{15}{subsection.2.4.2}\protected@file@percent }
\citation{DBLP:journals/cacm/ZhangBHRV21}
\citation{bartlett2020benign}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Projected Gradient Descent\relax }}{16}{algorithm.1}\protected@file@percent }
\newlabel{algorithm:targeted-pgd}{{6}{16}{Projected Gradient Descent\relax }{algorithm.1}{}}
\newlabel{algorithm:targeted-pgd@cref}{{[algorithm][1][]1}{[1][16][]16}}
\newlabel{algorithm:untargeted-pgd}{{13}{16}{Projected Gradient Descent\relax }{algorithm.1}{}}
\newlabel{algorithm:untargeted-pgd@cref}{{[algorithm][1][]1}{[1][16][]16}}
\newlabel{algorithm:targeted-fgsm}{{20}{16}{Projected Gradient Descent\relax }{algorithm.1}{}}
\newlabel{algorithm:targeted-fgsm@cref}{{[algorithm][1][]1}{[1][16][]16}}
\newlabel{algorithm:untargeted-fgsm}{{27}{16}{Projected Gradient Descent\relax }{algorithm.1}{}}
\newlabel{algorithm:untargeted-fgsm@cref}{{[algorithm][1][]1}{[1][16][]16}}
\citation{sanyal2021how}
\citation{sanyal2021how}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Benign Overfitting and label noise}{17}{section.2.5}\protected@file@percent }
\newlabel{section:benign}{{2.5}{17}{Benign Overfitting and label noise}{section.2.5}{}}
\newlabel{section:benign@cref}{{[section][5][2]2.5}{[1][16][]17}}
\newlabel{theorem:how-benign-bo}{{1}{17}{}{theorem.1}{}}
\newlabel{theorem:how-benign-bo@cref}{{[theorem][1][]1}{[1][17][]17}}
\citation{just-how-toxic,DBLP:journals/corr/abs-1712-05526}
\citation{DBLP:journals/corr/abs-1712-05526,hidden-trigger-backdoor,transferable-clean-label-poisoning}
\citation{label-flip-SVMs,certified-robustness}
\citation{transferable-clean-label-poisoning}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Dataset poisoning}{18}{section.2.6}\protected@file@percent }
\citation{sanyal2021how}
\citation{sanyal2021how}
\citation{sanyal2021how}
\citation{sanyal2021how}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Theoretical Results}{19}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{section:lipschitzness-assumption}{{3}{19}{Theoretical Results}{chapter.3}{}}
\newlabel{section:lipschitzness-assumption@cref}{{[chapter][3][]3}{[1][19][]19}}
\newlabel{eq:lipschitz}{{3}{19}{Theoretical Results}{chapter.3}{}}
\newlabel{eq:lipschitz@cref}{{[chapter][3][]3}{[1][19][]19}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Improved bound with Lipschitzness Assumptions}{19}{section.3.1}\protected@file@percent }
\newlabel{theorem:lipschitzness-extension}{{2}{19}{}{theorem.2}{}}
\newlabel{theorem:lipschitzness-extension@cref}{{[theorem][2][]2}{[1][19][]19}}
\citation{sanyal2021how}
\newlabel{eq:lipschitz-extension}{{3.1}{21}{Improved bound with Lipschitzness Assumptions}{Item.15}{}}
\newlabel{eq:lipschitz-extension@cref}{{[section][1][3]3.1}{[1][20][]21}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Lipschitzness bounded by distance to the nearest point, of different label}{21}{section.3.2}\protected@file@percent }
\newlabel{theorem:lipschitzness-extension-closest-one}{{3}{21}{}{theorem.3}{}}
\newlabel{theorem:lipschitzness-extension-closest-one@cref}{{[theorem][3][]3}{[1][21][]21}}
\citation{sanyal2021how}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Early Unsuccessful Experiments}{23}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Building graphs on training points}{23}{section.4.1}\protected@file@percent }
\newlabel{def:class-graph}{{1}{23}{}{definition.1}{}}
\newlabel{def:class-graph@cref}{{[definition][1][]1}{[1][23][]23}}
\citation{dominating-set}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Greedily Selecting vertices\relax }}{24}{algorithm.2}\protected@file@percent }
\newlabel{alg:greedy-selection}{{2}{24}{Greedily Selecting vertices\relax }{algorithm.2}{}}
\newlabel{alg:greedy-selection@cref}{{[algorithm][2][]2}{[1][24][]24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Building graphs on input space}{24}{subsection.4.1.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces $(\qopname  \relax o{log}\Delta + 2)$-approximation to the dominating set problem\relax }}{25}{algorithm.3}\protected@file@percent }
\newlabel{alg:log-delta}{{3}{25}{$(\log \Delta + 2)$-approximation to the dominating set problem\relax }{algorithm.3}{}}
\newlabel{alg:log-delta@cref}{{[algorithm][3][]3}{[1][24][]25}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Figure showing the distribution of the inter-point distances in class 0 of the MNIST dataset\relax }}{26}{figure.caption.6}\protected@file@percent }
\newlabel{fig:inter-point-distances}{{4.1}{26}{Figure showing the distribution of the inter-point distances in class 0 of the MNIST dataset\relax }{figure.caption.6}{}}
\newlabel{fig:inter-point-distances@cref}{{[figure][1][4]4.1}{[1][25][]26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Using Low Rank representations}{26}{subsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Using the feature representations of a trained network}{26}{subsection.4.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Using the feature representations of a trained network as well as input space}{26}{subsection.4.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Min Coloring trick}{27}{section.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Idea behind using different color labels for each point in a specific class, and getting a network to memorize the assigned labels. The black lines show the decision boundaries learnt for the different colors, and red boxes show the points which are vulnerable because of a boundary passing through the $l_\infty $ balls around those points.\relax }}{28}{figure.caption.7}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Experimental results}{29}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:experiments}{{5}{29}{Experimental results}{chapter.5}{}}
\newlabel{chapter:experiments@cref}{{[chapter][5][]5}{[1][29][]29}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Toy Distribution}{29}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Toy distribution with points far away from the decision boundary}{29}{subsection.5.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Toy data distribution with $d=2,\nobreakspace  {}\mu =10,\nobreakspace  {}\sigma ^2=1,\nobreakspace  {}d=2$\relax }}{30}{figure.caption.8}\protected@file@percent }
\newlabel{fig:data_sample}{{5.1}{30}{Toy data distribution with $d=2,~\mu =10,~\sigma ^2=1,~d=2$\relax }{figure.caption.8}{}}
\newlabel{fig:data_sample@cref}{{[figure][1][5]5.1}{[1][29][]30}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces \color@fb@x {}{orange}{}{orange}{\rule  {0pt}{3pt}\rule  {3pt}{0pt}} is the test accuracy \color@fb@x {}{blue}{}{blue}{\rule  {0pt}{3pt}\rule  {3pt}{0pt}} is the adversarial accuracy. $r$ axis represents the distance from the means of the gaussians where mislabelled points (poisons) were placed. Setting wrong labels to a third class produces a curve very similar to the case where labels were set to the opposite class for poisons. Multiple models were trained, and randomisation was also done on the poisons placed for each run, which is what the confidence intervals and mean curves are representative of.\relax }}{31}{figure.caption.9}\protected@file@percent }
\newlabel{fig:far-gaussian-curve}{{5.2}{31}{\fcolorbox {orange}{orange}{\rule {0pt}{3pt}\rule {3pt}{0pt}} is the test accuracy \fcolorbox {blue}{blue}{\rule {0pt}{3pt}\rule {3pt}{0pt}} is the adversarial accuracy. $r$ axis represents the distance from the means of the gaussians where mislabelled points (poisons) were placed. Setting wrong labels to a third class produces a curve very similar to the case where labels were set to the opposite class for poisons. Multiple models were trained, and randomisation was also done on the poisons placed for each run, which is what the confidence intervals and mean curves are representative of.\relax }{figure.caption.9}{}}
\newlabel{fig:far-gaussian-curve@cref}{{[figure][2][5]5.2}{[1][30][]31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Toy distribution with points close to the decision boundary}{31}{subsection.5.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces $d=64, b=2, \sigma ^2=1.0$; $2$ mislabeled points were placed per class. As before, randomisation is over training multiple models with different initialisations, and placing poisons randomly sampled at a given $r$ from the means.\relax }}{33}{figure.caption.10}\protected@file@percent }
\newlabel{fig:poison-toy-experiments}{{5.3}{33}{$d=64, b=2, \sigma ^2=1.0$; $2$ mislabeled points were placed per class. As before, randomisation is over training multiple models with different initialisations, and placing poisons randomly sampled at a given $r$ from the means.\relax }{figure.caption.10}{}}
\newlabel{fig:poison-toy-experiments@cref}{{[figure][3][5]5.3}{[1][32][]33}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}The worst poisons are not the ones producing pockets with the wrong label}{34}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Flipping to an 11th class in MNIST}{34}{subsection.5.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Adversarial Accuracies of networks when poisoning is done to a new class compared to the case when poisoning is done to the next class cyclically. The accuracy was evaluated against an $l_\infty $ PGD adversary with an attack radius of $0.1$. Randomisation in the above results is over model initialisation. For each run, the same points in MNIST are label-flipped.\relax }}{34}{table.caption.11}\protected@file@percent }
\newlabel{table:mnist-class-10}{{5.1}{34}{Adversarial Accuracies of networks when poisoning is done to a new class compared to the case when poisoning is done to the next class cyclically. The accuracy was evaluated against an $l_\infty $ PGD adversary with an attack radius of $0.1$. Randomisation in the above results is over model initialisation. For each run, the same points in MNIST are label-flipped.\relax }{table.caption.11}{}}
\newlabel{table:mnist-class-10@cref}{{[table][1][5]5.1}{[1][34][]34}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Back to MNIST: Analysis using adversarial paths}{35}{section.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces The above figure shows two different ways in which a classifier could memorize a mislabeled point. If the decision boundary is tweaked to accommodate a mislabeled point, it could lead to much higher adversarial errors, compared to the other case.\relax }}{35}{figure.caption.12}\protected@file@percent }
\newlabel{fig:pockets-or-tweaks}{{5.4}{35}{The above figure shows two different ways in which a classifier could memorize a mislabeled point. If the decision boundary is tweaked to accommodate a mislabeled point, it could lead to much higher adversarial errors, compared to the other case.\relax }{figure.caption.12}{}}
\newlabel{fig:pockets-or-tweaks@cref}{{[figure][4][5]5.4}{[1][34][]35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Finding a path from a point to the boundary}{35}{subsection.5.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Using adversarial paths to analyse poisons}{36}{subsection.5.3.2}\protected@file@percent }
\newlabel{subfig:close}{{5.5a}{37}{Subfigure 5 5.5a}{subfigure.5.5.1}{}}
\newlabel{subfig:close@cref}{{[subfigure][1][5,5]5.5a}{[1][36][]37}}
\newlabel{sub@subfig:close}{{(a)}{a}{Subfigure 5 5.5a\relax }{subfigure.5.5.1}{}}
\newlabel{subfig:far}{{5.5b}{37}{Subfigure 5 5.5b}{subfigure.5.5.2}{}}
\newlabel{subfig:far@cref}{{[subfigure][2][5,5]5.5b}{[1][36][]37}}
\newlabel{sub@subfig:far}{{(b)}{b}{Subfigure 5 5.5b\relax }{subfigure.5.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces After finding out the adversarial paths from each training point to the decision boundary of a model trained on MNIST, we visualized some points closest to and farthest from the decision boundary. Examples pertaining to each class have been put on different rows: 0 to 9 from top to bottom. Note that the path to the next class was found by untargeted-unbounded-gradient-ascent, which is why label noise is such that some of the 0s look like 1s, 1s like 2s, and so on. Constratingly, the images on the right are far away from the boundary, again evaluated in a targeted manner. These images seem very crisp.\relax }}{37}{figure.caption.13}\protected@file@percent }
\newlabel{fig:pgd-path-examples}{{5.5}{37}{After finding out the adversarial paths from each training point to the decision boundary of a model trained on MNIST, we visualized some points closest to and farthest from the decision boundary. Examples pertaining to each class have been put on different rows: 0 to 9 from top to bottom. Note that the path to the next class was found by untargeted-unbounded-gradient-ascent, which is why label noise is such that some of the 0s look like 1s, 1s like 2s, and so on. Constratingly, the images on the right are far away from the boundary, again evaluated in a targeted manner. These images seem very crisp.\relax }{figure.caption.13}{}}
\newlabel{fig:pgd-path-examples@cref}{{[figure][5][5]5.5}{[1][36][]37}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Targeted Unbounded Gradient Ascent\relax }}{38}{algorithm.4}\protected@file@percent }
\newlabel{algorithm:unbounded-gd}{{8}{38}{Targeted Unbounded Gradient Ascent\relax }{algorithm.4}{}}
\newlabel{algorithm:unbounded-gd@cref}{{[algorithm][4][]4}{[1][38][]38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}The experiments}{38}{subsection.5.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{0-1 MNIST}{38}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{MNIST}{38}{section*.16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces (a) shows the distribution over the number of gradient ascent updates it took for the classifier to flip its output, for 0-1 MNIST, for the points in the training set. The $y$-axis indicates the number of points in the training set. The step size used for the above was 0.01, on $[0, 1]$ normalized input images. (b) The $x$ axis only denotes an ordinal value: higher values represent buckets holding points for which the adversarial paths were lengthier. The points were segregated into different buckets for (b), each bucket having points at a certain distance range from the boundary. The test accuracy of all of the above models are greater than 99\%.\relax }}{39}{figure.caption.15}\protected@file@percent }
\newlabel{fig:0-1-mnist-pgd-path}{{5.6}{39}{(a) shows the distribution over the number of gradient ascent updates it took for the classifier to flip its output, for 0-1 MNIST, for the points in the training set. The $y$-axis indicates the number of points in the training set. The step size used for the above was 0.01, on $[0, 1]$ normalized input images. (b) The $x$ axis only denotes an ordinal value: higher values represent buckets holding points for which the adversarial paths were lengthier. The points were segregated into different buckets for (b), each bucket having points at a certain distance range from the boundary. The test accuracy of all of the above models are greater than 99\%.\relax }{figure.caption.15}{}}
\newlabel{fig:0-1-mnist-pgd-path@cref}{{[figure][6][5]5.6}{[1][38][]39}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Distribution over the number of gradient ascent updates it took for the classifier to flip its output, for the full MNIST dataset, for the points in the training set. The $y$-axis indicates the number of points in the training set. The step size used for the above was 0.01, on $[0, 1]$ normalized input images. Note that for class "1", it takes not more than 10 steps for almos the entire training set, which means for an attack radius of $\epsilon =0.1$, the entire class must be vulnerable, which is indeed the case. The adversarial accuracy of the model trained on the clean dataset, on targeted attacks from 1 to 2 is 0.\relax }}{40}{figure.caption.17}\protected@file@percent }
\newlabel{fig:full-mnist-pgd-path}{{5.7}{40}{Distribution over the number of gradient ascent updates it took for the classifier to flip its output, for the full MNIST dataset, for the points in the training set. The $y$-axis indicates the number of points in the training set. The step size used for the above was 0.01, on $[0, 1]$ normalized input images. Note that for class "1", it takes not more than 10 steps for almos the entire training set, which means for an attack radius of $\epsilon =0.1$, the entire class must be vulnerable, which is indeed the case. The adversarial accuracy of the model trained on the clean dataset, on targeted attacks from 1 to 2 is 0.\relax }{figure.caption.17}{}}
\newlabel{fig:full-mnist-pgd-path@cref}{{[figure][7][5]5.7}{[1][40][]40}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Full MNIST dataset. The $x$ axis only denotes an ordinal value: higher values represents represents buckets holding points for which the adversarial paths were lengthier. The orange line is a baseline showing the adversarial accuracy when same number of points are randomly selected to poison. The 3 graphs shown are for classes 1, 4, and 9 flipped to 2, 5, and 0 respectively. These classes are at increasing distances from the decision boundary: points in class 1 are very close, and points in class 9 are very far, class 4 being an intermediate case. The adversarial accuracy evaluation for each case is done using targeted attacks from classes 1, 4, and 9 to 2, 5, 0 respectively to study the effects of poisons. 10 poisons were placed per class for these runs. The test accuracy of all of the networks evaluated in the above diagram is more than 99\%.\relax }}{41}{figure.caption.18}\protected@file@percent }
\newlabel{fig:full-MNIST-curve}{{5.8}{41}{Full MNIST dataset. The $x$ axis only denotes an ordinal value: higher values represents represents buckets holding points for which the adversarial paths were lengthier. The orange line is a baseline showing the adversarial accuracy when same number of points are randomly selected to poison. The 3 graphs shown are for classes 1, 4, and 9 flipped to 2, 5, and 0 respectively. These classes are at increasing distances from the decision boundary: points in class 1 are very close, and points in class 9 are very far, class 4 being an intermediate case. The adversarial accuracy evaluation for each case is done using targeted attacks from classes 1, 4, and 9 to 2, 5, 0 respectively to study the effects of poisons. 10 poisons were placed per class for these runs. The test accuracy of all of the networks evaluated in the above diagram is more than 99\%.\relax }{figure.caption.18}{}}
\newlabel{fig:full-MNIST-curve@cref}{{[figure][8][5]5.8}{[1][40][]41}}
\citation{maml,first-order-meta,forward-reverse-hyperparameter}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Future Work: A Meta-Learning Approach}{42}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{DBLP:conf/iclr/MaddisonMT17}
\newlabel{eq:optimization}{{6.1}{43}{Future Work: A Meta-Learning Approach}{equation.6.0.1}{}}
\newlabel{eq:optimization@cref}{{[equation][1][6]6.1}{[1][43][]43}}
\newlabel{eq:optimization-prac}{{6.2}{43}{Future Work: A Meta-Learning Approach}{equation.6.0.2}{}}
\newlabel{eq:optimization-prac@cref}{{[equation][2][6]6.2}{[1][43][]43}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces An example of the multi-level optimisation case, where we have a small dataset of 8 examples, and two gradient updates are made in the innermost loop, with each batch update having 4 examples. The function get\_adversarial\_examples () produces adversarial exapmles for a given network and inputs. Dotted lines indicate that the operations are not to be recorded, and bold lines indicate that the operations should be recorded, so that we can pass gradients through them during the backward pass. $\odot $ refers to elementwise multiplication.\relax }}{45}{figure.caption.19}\protected@file@percent }
\newlabel{fig:bilevel-opt}{{6.1}{45}{An example of the multi-level optimisation case, where we have a small dataset of 8 examples, and two gradient updates are made in the innermost loop, with each batch update having 4 examples. The function get\_adversarial\_examples () produces adversarial exapmles for a given network and inputs. Dotted lines indicate that the operations are not to be recorded, and bold lines indicate that the operations should be recorded, so that we can pass gradients through them during the backward pass. $\odot $ refers to elementwise multiplication.\relax }{figure.caption.19}{}}
\newlabel{fig:bilevel-opt@cref}{{[figure][1][6]6.1}{[1][44][]45}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion}{46}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibstyle{plainnat}
\bibdata{ref}
\bibcite{dominating-set}{{1}{}{{dom}}{{}}}
\bibcite{bartlett2020benign}{{2}{2020}{{Bartlett et~al.}}{{Bartlett, Long, Lugosi, and Tsigler}}}
\bibcite{belkin2018understand}{{3}{2018}{{Belkin et~al.}}{{Belkin, Ma, and Mandal}}}
\bibcite{adv-vuln-in-mission-critical}{{4}{2019}{{Boloor et~al.}}{{Boloor, He, Gill, Vorobeychik, and Zhang}}}
\bibcite{DBLP:journals/corr/abs-1712-05526}{{5}{2017}{{Chen et~al.}}{{Chen, Liu, Li, Lu, and Song}}}
\bibcite{maml}{{6}{2017}{{Finn et~al.}}{{Finn, Abbeel, and Levine}}}
\bibcite{forward-reverse-hyperparameter}{{7}{2017}{{Franceschi et~al.}}{{Franceschi, Donini, Frasconi, and Pontil}}}
\bibcite{Goodfellow2015ExplainingAH}{{8}{2015}{{Goodfellow et~al.}}{{Goodfellow, Shlens, and Szegedy}}}
\bibcite{imagenet-superhuman}{{9}{2015}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{hornik1989multilayer}{{10}{1989}{{Hornik et~al.}}{{Hornik, Stinchcombe, and White}}}
\bibcite{Adam}{{11}{2015}{{Kingma and Ba}}{{}}}
\bibcite{imagenet}{{12}{2012}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever, and Hinton}}}
\bibcite{neco.1989.1.4.541}{{13}{1989}{{LeCun et~al.}}{{LeCun, Boser, Denker, Henderson, Howard, Hubbard, and Jackel}}}
\bibcite{DBLP:conf/iclr/MaddisonMT17}{{14}{2017}{{Maddison et~al.}}{{Maddison, Mnih, and Teh}}}
\bibcite{madry2019deep}{{15}{2018}{{Madry et~al.}}{{Madry, Makelov, Schmidt, Tsipras, and Vladu}}}
\bibcite{first-order-meta}{{16}{2018}{{Nichol et~al.}}{{Nichol, Achiam, and Schulman}}}
\bibcite{certified-robustness}{{17}{2020}{{Rosenfeld et~al.}}{{Rosenfeld, Winston, Ravikumar, and Kolter}}}
\bibcite{hidden-trigger-backdoor}{{18}{2020}{{Saha et~al.}}{{Saha, Subramanya, and Pirsiavash}}}
\bibcite{sanyal2021how}{{19}{2021}{{Sanyal et~al.}}{{Sanyal, Kanade, Dokania, and Torr}}}
\bibcite{just-how-toxic}{{20}{2020}{{Schwarzschild et~al.}}{{Schwarzschild, Goldblum, Gupta, Dickerson, and Goldstein}}}
\bibcite{alpha-zero}{{21}{2017}{{Silver et~al.}}{{Silver, Schrittwieser, Simonyan, Antonoglou, Huang, Guez, Hubert, Baker, Lai, Bolton, Chen, Lillicrap, Hui, Sifre, Driessche, Graepel, and Hassabis}}}
\bibcite{seq2seq}{{22}{2014}{{Sutskever et~al.}}{{Sutskever, Vinyals, and Le}}}
\bibcite{42503}{{23}{2014}{{Szegedy et~al.}}{{Szegedy, Zaremba, Sutskever, Bruna, Erhan, Goodfellow, and Fergus}}}
\bibcite{attention-is-all-you-need}{{24}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{starcraft}{{25}{2019}{{Vinyals et~al.}}{{Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik, Chung, Choi, Powell, Ewalds, Georgiev, Oh, Horgan, Kroiss, Danihelka, Huang, Sifre, Cai, Agapiou, Jaderberg, Vezhnevets, Leblond, Pohlen, Dalibard, Budden, Sulsky, Molloy, Paine, G{\"{u}}l{\c {c}}ehre, Wang, Pfaff, Wu, Ring, Yogatama, W{\"{u}}nsch, McKinney, Smith, Schaul, Lillicrap, Kavukcuoglu, Hassabis, Apps, and Silver}}}
\bibcite{label-flip-SVMs}{{26}{2012}{{Xiao et~al.}}{{Xiao, Xiao, and Eckert}}}
\bibcite{DBLP:journals/cacm/ZhangBHRV21}{{27}{2021}{{Zhang et~al.}}{{Zhang, Bengio, Hardt, Recht, and Vinyals}}}
\bibcite{transferable-clean-label-poisoning}{{28}{2019}{{Zhu et~al.}}{{Zhu, Huang, Li, Taylor, Studer, and Goldstein}}}
