Crafting Label Noise
Sharan Gopal
August 5, 2021

1

The Problem

Deep Neural Networks have gained a lot of popularity in the Machine Learning Community, because of the fact that they are universal approximators,
coupled with improving training algorithms, and an abundance of data and
powerful hardware. These useful properties have helped deep learning achieve
the state of the art in many tasks, from computer vision, machine translation, to achieveing superhuman performance in previously unsolved board or
computer games.
Recent works [Belkin et al., 2018, Bartlett et al., 2020] pointed out that
several over-parameterized machine learning models can memorize label noise
(mislabelled points), which is surprisingly already present in widely used
datasets such as MNIST and CIFAR-10, without showing a noticable drop
in their test accuracies. This has been investigated with artificially induced
label noise in standard datasets as well, and is referred to as benign overfitting.
Although, memorizing noisy data does come with some downsides. [Sanyal
et al., 2021] investigated the consequences of memorizing uniform label noise
on the adversarial error of the model and showed that this will increase in
the presense of such label noise, because the model becomes increasingly vulnerable in regions where a wrong label has been memorized. So a model that
is trained on data with label noise might perform very well on unseen test
data, but it will be highly adversarially vulnerable, much more than what a
model trained on clean data would be. This means, we, as attackers can add
human imperceptible perturbations to the input of a network, and get the
network to misclassify, and if a human is present in the loop observing the
input, they would not notice the attack.

1

While [Sanyal et al., 2021] experimentally and theoretically investigates
a setting where models memorize uniform label noise, there has not been
any study towards carefully crafting label noise. Further, [Sanyal et al.,
2021] theoretically analyse the poisoning phenomenon where networks are
able to memorize label noise using dictionaries, attaining the lowest possible
test accuracy, and they produce bounds for the adversarial accuracy in such
cases. In practice, networks learned with SGD are much smoother, and end
up learning regions around these mislabelled points where the network will
produce the wrong label, rather than at just the mislabelled points.
Is it the case that changing the labels of certain points in the dataset worse
for the adversarial accuracy of neural networks than others? The problem at
hand is in the following direction. Given a budget Î·, i.e. if we are allowed
to change the labels of Î·|D| points where D is the dataset, the attempt is
i=n
to study functions F(D) that take in a training dataset D = {(xi , yi )}i=1
and produce a poisoned dataset DÌƒ with DÌƒx,. = Dx,. and |DÌƒ \ D| â‰¤ Î·|D| (i.e.,
DÌƒ has the same datapoints, with at max Î·|D| labels flipped), such that a
model trained on DÌƒ ends up being highly adversarially vulnerable, at least
with respect to the random-uniform strategy. Victims who might train their
models on this poisoned dataset will not notice a drop in the test accuracy,
but the adversarial accuracy of their model will be very poor. When their
model gets deployed, we, as attackers will know the points of vulnerability in
the victimâ€™s model, since we injected label noise. This study is in the hope
that we can uncover some properties of neural networks in general, and what
effects memorizing label noise has on the network, so that we might be able
to defend against memorizing label noise.
When mislabeled points are introduced into the dataset, deep neural networks trained on the dataset learn regions where they misclassify the input,
and the aim is to also study the nature of these regions. Are they seperate
pockets, such that nearby points become vulnerable, or are they extensive,
far reaching regions which render points far away from the mislabeled training point vulnerable? Many of these questions have not been answered by
existing literature.

2

2
2.1

Background
The supervised learning problem

The problem of supervised learning is as follows: given a finite set D âŠ† Rd
sampled from a distribution P, and labeled using a function c : Rd â†’ â„¦,
where â„¦ is a set of classes, we are to find a function C : Rd â†’ â„¦ which
can label unseen data sampled from the same distribution P, reliably as c
would have labeled. This notion of reliability is captured formally through
the natural risk.
The natural risk RNat (C) of a classifier C : Rd â†’ â„¦ in a classification
problem given by the data distribution/density function P : Rd â†’ Râ‰¥0 where
d is the data dimensionality, and ground truth labeling function c : Rd â†’ â„¦,
where â„¦ is the set of classes, is defined as
Z
RNat (C) = Pxâˆ¼P [c(x) 6= C(x)] =
P(x) Jc(x) 6= C(x)K dx
(1)
Rd

In other words, the natural risk is the probability of the classifier getting a
wrong answer. In practice, we carry out a Monte-Carlo estimation of the
natural risk, by finding the fraction of points in an unseen set of points
sampled i.i.d. from P (called the â€test setâ€) for which the classifier gets the
incorrect output. This estimate is also called the â€test errorâ€.

2.2

Neural networks

Deep Neural Networks (DNNs) are a family of highly parametrized statistical
models, that can be trained to approximate any function required. They
are proven universal approximators [Hornik et al., 1989], which means with
enough parameters, they can approximate any function with arbitrarily small
error.
DNNs are composed of layers, with each layer parameterized by weights
and biases. Each layer takes in a vector and produces another vector. This
is done by first doing an affine transformation of the input vector, which is
entirely specified by the weight matrix and bias of that layer. This transformation yields a vector which is then passed through an activation function.
DNNs acquire their non-linearity through these activation functions. More
precisely, let the ith layer of a network be denoted as Li : Rdi â†’ Rdi+1 . Let

3

its weight matrix be Wi âˆˆ Rdi Ã—di+1 , bias be b âˆˆ Rdi+1 and activation function
be Ïƒ. Then the layer Li is defined as the following function:
Li (x) = Ïƒ(Wi x + b)
Then the entire DNN is simply a composition of all of its layers. So a
DNN C : Rdin â†’ Rdin having k layers L1 , L2 ... to Lk is defined as:
C(xin ) = Lk â—¦ Lkâˆ’1 ... â—¦ L1 (xin )
The series of computations we perform while finding the outputs of each layer
for a given input is called a forward pass.
The ReLU activation function is the most popular one. It is defined as

0
xâ‰¤0
ReLU(x) =
x
x>0
2 other activation functions are the sigmoid and tanh activation functions
sigmoid(x) =

1
1 + eâˆ’x

and
tanh(x) = 2 Â· Ïƒ(x) âˆ’ 1
The above definitions involve scalar inputs to these functions. We will overload vector inputs for the activation functions defined above, such that each
dimension in the input vector is passed through the activation function considered.
For classification problems, we require the network to output a probability
distribution over classes â„¦. This is achieved in design by using the softmax
activation function, defined as:
exi
softmax : Rd â†’ Rd ; softmax(x) = x0 s.t. x0 i = Pd
j=1

exj

The softmax function is essentially a soft version of the argmax function,
which returns a one-hot vector with a 1 at the index with the maximal
value in a vector. The above activation produces a probability distribution
over each dimension. Hence, in networks used for classification problems, we
generally activate the final layer, having |â„¦| dimensional outputs, with the
softmax activation function.
4

For each layer, the output dimensionality is a design choice. Any design
choice, be it the number of layers or the activation functions used, are inductive biases. We assume that the best function we are looking for lies in a
family of networks with the design we fix, with each member having a different parameter set. Now, we will need to search over this family of functions
for the â€bestâ€ one, having certain values for its parameters, which is what
the training algorithm does.
Convolutional Neural Networks (CNNs) are DNNs that perform very well
on image data. Briefly, we have filters at each layer, which are convoluted
with the input image produced by the previous layer. These feature maps
capture spacial correlations in the image. All image-dataset experiments
done in this thesis utilise CNNs.
2.2.1

Stochastic Gradient Descent

Stochastic Gradient Descent (SGD) is an optimisation technique that assumes a first order approximation of the objective function. Given a function
f (x), which needs to be minimised over x, gradient descent attempts to find
a minimum by iteratively updating the current best as:
xi+1 = xi âˆ’ Î± Â· âˆ‡x f (x)

x=xi

where Î± is a hyperparameter called the learning rate. Hence, SGD requires
gradients of f computed w.r.t x.
We can also use second order optimisation methods, but these are infeasible to implement in practice due to the high computational cost of calculating
second order gradients (Hessians). There are a few popular modifications of
SGD, the most popular one being the Adam optimizer. Adam utilises the
gradients given to it at each update step to approximate second order gradient, and converges much faster than vanilla SGD.
2.2.2

Loss functions

To train networks, we need to have a metric that captures how â€goodâ€ a network is. We could then optimise over this metric, and tweak the parameters
of the model hoping that the resultant model will have a low natural risk.
Such metrics are called loss functions.
For classification problems, including the ones in this thesis, we will be
utilising the Cross Entropy Loss, which is standard for classifiers. For a
5

â€targetâ€ distribution T , and a distribution D both over a set â„¦, the cross
entropy loss produces the lowest value when T = D, and has high values
when D is very different from T .
X
H[D, T ] = âˆ’
T (Ï‰) log D(Ï‰)
Ï‰âˆˆâ„¦

2.2.3

Backpropagation

To train a neural network C with the algorithms mentioned in the section 2.2.1, we require to compute Exâˆ¼P [âˆ‡Î¸ L(C(x), c(x))] where P is the
data distribution. In practice, this is done by sampling a batch of b samples
X = [xi | i âˆˆ [1..b]] uniformly from the test set, and computing
Pi=b
1
i=1 âˆ‡Î¸ L(C(xi ), c(xi )).
b
In order to do this computation in neural networks, we require to methodically apply the chain rule. The backpropagation algorithm does this.
We first record all computations we do during the forward pass, and the
following recursive relation is utilised to find the gradients of the loss with
respect to one value in this forward pass. Let this value be v, and all values
dependent on v be u1 ...uk . Let L denote the final value whose gradient we
âˆ‚L
âˆ‚L
... âˆ‚u
and we know
are to compute, w.r.t. v, assuming we already have âˆ‚u
1
k
âˆ‚v
how to compute âˆ‚ui . Then,
b

âˆ‚L X âˆ‚v âˆ‚L
=
Â·
âˆ‚v
âˆ‚ui âˆ‚ui
i=1
So we start from âˆ‚L
= 1, and work â€backwardsâ€ from there to compute
âˆ‚L
all gradients.
The relation above can be applied to any network such that for each
operation, we can compute gradients of the operationâ€™s outputs w.r.t. its
inputs, and allows for far more flexibility than than just working for the
DNN architecture described above.

2.3

Adversarial risk

[Szegedy et al., 2014] discovered a significant property of deep neural networks: they are very sensitive to changes in the input. Networks trained on
data drawn from a distribution perform very well on unseen data from the
6

same distribution, for example that of handwritten digits, but are extremely
sensitive to small, almost human imperceptible input perturbations. This is
generally referred to in literature as Adversarial Vulnerability. [Goodfellow
et al., 2015] and [Madry et al., 2018] showed some methods to craft such
perturbations, and these are called adversarial attacks, and will be discussed
later.
This sensitivity to input perturbations is a significant vulnerability, since
the changes in the input are mostly imperceptible to humans, and raises
questions about the robustness and reliability of DNNs deployed in the real
world, specifically in critical applications, such as authentication.
The adversarial vulnerability of a classifier C is quantified and formalised
Ï
through the adversarial risk. The adversarial risk RAdv
(C) of a classifier
C : Rd â†’ â„¦ in a classification problem given by the data distribution/density
function P : Rd â†’ Râ‰¥0 where d is the data dimensionality, and ground truth
labeling function c : Rd â†’ â„¦, where â„¦ is the set of classes, is defined as
Ï
RAdv
(C) = Pxâˆ¼P [âˆƒxÌƒ âˆˆ BÏp (x) s.t. c(x) 6= C(xÌƒ)]
Z
=
P(x) JâˆƒxÌƒ âˆˆ BÏp (x) s.t. c(x) 6= C(xÌƒ)K dx

(2)

Rd

where

BÏp (x)

is the set of all points inside an lp ball around x, given by
BÏp (x) = {x0 âˆˆ Rd | Ï â‰¥ kx0 âˆ’ xkp }

for some norm p.
The adversarial risk intuitively captures the probability that we get an
input such that it can be perturbed within a constrained region, so that the
classifier gives the wrong output. In practice, this risk is estimated by utilizing some standard attack-strategy to calculate perturbations, such as those
explained next, and finding the fraction of test-set points for which we could
successfully find a perturbation that could make the classifier misclassify.
Given a classifier C and a point x âˆ¼ P, the problem of finding an adversarial example for x, i.e. finding an xÌƒ âˆˆ BÏp (x) s.t. c(x) 6= C(xÌƒ) can be
written as a constrained optimization problem:
xÌƒ = arg max L(C(u), c(x))
uâˆˆBÏp (x)

where L is a loss function, and could be the same one used to train the
model. So we maximize this loss inside the lp ball around x to find an
7

adversarial example. The solution to the above optimisation problem is an
untargeted adversarial example. This means that the resultant adversarial
example could be classified by the model as any wrong class, which is why
we call algorithms that solve or approximate the above as untargeted attacks.
We could also have targeted attacks, which provide greater control to
attackers. Targeted attacks involve creating adversarial examples such that
the classifier classifies it to be of a particular target class. Let the target
class be t. Then the constrained optimisation problem for a targeted attack
around x is
xÌƒt = arg min L(C(u), t)
uâˆˆBÏp (x)

The brief review on attack strategies that follows this discussion is based
on different ways to go about computationally solving the above maximization problem.

(a) Classified as a â€5â€

(b) Classified as a â€3â€

Figure 1: A convolutional neural network trained on the MNIST dataset,
attained 99.19% test accuracy. The classifier predicts the image of â€5â€ on
the left correctly with 100% confidence. The same â€5â€ was perturbed, within
a 0.04 lâˆ radius of the original â€5â€. The same classifier classifies this â€5â€ as
a â€3â€, with more than 60% confidence. This classifier has a 0% adversarial
accuracy for an 0.3 lâˆ radius, which means for each point in the test set, we
can find a small enough perturbation to add to the original image, and get
the classifier to misclassify. The reader might need to view the page from
different angles to perceive the difference between the two images.

8

2.4

Standard attacks for neural networks in literature

Here, we discuss 2 popular attack techniques in literature. When we use
the term attack, the problem we attempt to solve is finding an adversarial
example, i.e., given a point from the data distribution, we are to find out a
point within an lp ball (of some radius Ï) around that point, such that the
model gives the wrong output for that point. For a targeted attack, given a
point x with label c(x) from the data distribution, we are to find a point x0
inside an lp ball around x such that the model classifies x0 as a target class
t.
Reiterating, our objective will be to solve a constrained optimisation problem, minimising or maximising the loss, depending on whether or not we are
doing a targeted, or untargeted attack.
Let L : Î“(â„¦) Ã— â„¦ â†’ R be the loss function considered, where â„¦ is the set
of classes and Î“(â„¦) is the set of all probability distributions over â„¦. For our
discussion, it does not matter which loss is used, as long as it penalizes, or
has high values when the model produces the wrong class prediction, and low
value when it gives the right one. The cross entropy loss is the most popular
choice. Given a data-point x and neural network classifier CÎ¸ parameterized
by Î¸, these methods rely on computing the loss value by forward propagating
through CÎ¸ , and then backpropagating, computing gradients all the way to
the inputs, finding the gradient of the loss with respect to the input.
Essentially, we compute the following:
âˆ†x = âˆ‡p L(CÎ¸ (p), y)

p=x

we use the above computed value to carry out gradient ascent for untargeted attacks, and gradient descent for targeted attacks, setting y = t
where t is the target class. Since we require access to the model and all the
computations done by it to compute the gradient above, these attacks are
also referred to as white box gradient based attacks. There are many black
box attacks as well, where access to the model is not assumed. These white
box attacks are however, out of the scope of our discussion.
2.4.1

Fast Gradient Sign Method

The Fast Gradient Sign Method (FGSM), is a one step procedure, and requires only one forward-backward pass through the network. The norm constraint assumed for the attack is  in lâˆ norm, which allows the attacker
9

to produce adversarial examples within an  radius lâˆ ball around x. The
adversarial example produced by FGSM is given by:
x0 = x +  Â· sign(âˆ‡p L(CÎ¸ (p), y)

p=x

)

for an untargeted attack, and
x0 = x âˆ’  Â· sign(âˆ‡p L(CÎ¸ (p), t)

p=x

)

for a targeted attack.
2.4.2

Projected Gradient Descent

The Projected Gradient Descent (PGD) attack is the most popular adversarial attack in literature. Throughout this thesis, wherever we refer to attacks
in experiments, we refer to PGD attacks. In literature, and in this thesis as
well, whenever we have to evaluate the adversarial accuracy of a network,
we iterate through the test set, and find out the fraction of the test set for
which the PGD attack was unsuccessful.
The PGD attack involves a few hyperparameters:
1. Attack-steps m: This is the number of iterations the PGD algorithm
is going to take
2. Step-size Âµ: This is the size of each update the algorithm will take. The
impact of this hyperparameter will be shown in the update equation
below
3. Attack-norm p: Perturbations are constrained to be within a region
around the input x. This distance constraint is in the form of a norm,
which we will call the attack-norm. For instance, perturbations could
be constrained within an lâˆ ball (p = âˆ) of the input.
4. Attack-radius : This is the radius of the lp ball within which the
perturbed image should lie around the original image.
We repreatedly perturb the current perturbed image using FGSM, and project
it back onto the lp ball around the original input.
The PGD algorithm is as shown in algorithm 13.

10

Algorithm 1 Projected Gradient Descent
1:
2:
3:
4:
5:

procedure Targeted-PGD(CÎ¸ , x, t, , p, m, Âµ)
x0 â† x
for i â† [1..m] do
xi = Targeted-FGSM(CÎ¸ , xiâˆ’1 , t, Âµ)
xi â† Project(xi , BÏp (x))
. Project onto lp ball around x
return xm

6:
7:
8:
9:
10:
11:
12:

procedure Untargeted-PGD(CÎ¸ , x, y, , p, m, Âµ)
x0 â† x
for i â† [1..m] do
xi = Untargeted-FGSM(CÎ¸ , xiâˆ’1 , y, Âµ)
. Project onto lp ball around x
xi â† Project(xi , BÏp (x))

13:

return xm

14:
15:
16:
17:
18:

procedure Targeted-FGSM(CÎ¸ , x, t, )
d â† CÎ¸ (x)
. Forward pass. d âˆˆ Î“(â„¦) is the networkâ€™s output
` â† L(d, t)
. Operations are recorded to support backward-pass
âˆ†x = âˆ‡p L(CÎ¸ (p), t) p=x
. Backpropagate
0
19:
x = x âˆ’  Â· sign(âˆ†x)
20:
return x0
21:
22:
23:
24:
25:

procedure Untargeted-FGSM(CÎ¸ , x, y, )
d â† CÎ¸ (x)
. Forward pass. d âˆˆ Î“(â„¦) is the networkâ€™s output
` â† L(d, t)
. Operations are recorded to support backward-pass
âˆ†x = âˆ‡p L(CÎ¸ (p), y) p=x
. Backpropagate
0
26:
x = x +  Â· sign(âˆ†x)
27:
return x0

2.5

Adversarial Training

PGD training Speak a bit about TRADES

11

2.6

3

Benign Overfitting and label noise

Some Theory

3.1

Extending the Result with Lipschitzness Assumptions

[Sanyal et al., 2021] showed a theoretical result regarding uniform label noise,
stated above in 2.6. The proof assumes the best case scenario where the network trained on poisoned data memorizes mislabeled points discontinuously,
as dictionaries. This will have no effect on the test accuracy of the network,
and points only in an lp ball around the mislabelled points will be vulnerable. Practically, networks are smooth, and this smoothness can be formalized
using Lipschitzness assumptions.
A function f : X â†’ Y is locally lipschitz-continuous if there exists a
constant Îº s.t. âˆ€x âˆˆ X, there exists a neighbourhood U around x, s.t.
âˆ€x1 , x2 âˆˆ U ,
kf(x1 ) âˆ’ f(x2 )kq
â‰¤Îº
kx1 âˆ’ x2 kp
for some distance metrics k.kp and k.kq .
Deep Neural Networks are lipschitz, and including this assumption in
Theorem-1 of [Sanyal et al., 2021], gives the same bounds for a weaker adversary, i.e., we require a smaller attack-radius to get the same lower bound
on the adversarial risk obtained by [Sanyal et al., 2021]. Below is the extended theorem, with the same assumptions (restated) from [Sanyal et al.,
2021], with the additional assumption of lipschitzness.
Theorem 1. Let C be a classifier, and D : Rd â†’ Râ‰¥0 the data distribution,
and c : Rd â†’ â„¦ be the ground truth labeling function, where â„¦ is the set of
classes. If there exist constants c1 â‰¥ c2 > 0, Ï > 0 and a set Î¶ âŠ‚ Rd , such
that


S p
1. Pxâˆ¼D
BÏ (s) â‰¥ c1
sâˆˆÎ¶

2. âˆ€s âˆˆ Î¶, Pxâˆ¼D [BÏp (s)] â‰¥

c2
|Î¶|

3. âˆ€s âˆˆ Î¶, âˆ€u, v âˆˆ BÏp (s), c(u) = c(v)
12

where BÏp (x) is a Ï-radius lp ball around x. Let S be a training set, obtained by
taking n i.i.d. samples from P, and labeling each with c(.) with Î· probability,
or some incorrect class with probability 1 âˆ’ Î·. If a classifier C is such that
1. C has a 100% training accuracy on S. This means it gets the correct class on unflipped points, and has also memorized the wrong label
on flipped points. Further, on each training point, the classifier C is
confident on the result at least Ï‡ more than the second largest output
probability.
2. C is locally lipschitz, so for each point x âˆˆ Rd and x0 in its neighbourhood,
JC(x) 6= C(x0 )K
â‰¤Îº
kx âˆ’ x0 kp
then if n â‰¥
Ï„ = Îº1 .

|Î¶|
Î·c2

log(|Î¶|/Î´), with at least 1 âˆ’ Î´ probability, R2Ïâˆ’Ï„
Adv (C) â‰¥ c1 where

Proof. Consider some x âˆˆ Î¶, s.t. âˆƒ(x0 , y) âˆˆ S, x0 âˆˆ BÏp (x), y 6= c(x0 ), i.e.,
there exists a training point in S inside the lp ball around x which has been
mislabeled. The classifier would have memorized this point. We need to
find out the attack radius Ï required to render all of the points inside BÏp (x)
vulnerable. The worst case is when this point x0 lies on the surface of BÏp (x)
since this is the case where the attack radius required is going to be the
largest. Now, any mislabeled point that is memorized by the classifier is
going to create a region of radius at least Ï„ where the classifier produces
the same wrong label y. This means in the worst case, when x0 lies on the
surface, we will require an attack radius of 2Ï âˆ’ Ï„ for all points in BÏp (x) to
be vulnerable.
If there is one mislabeled point placed inside each lp ball around every
point in Î¶, then R2Ïâˆ’Ï„
Adv â‰¥ c1 , since all of the mass inside every lp ball considered contributes to the adversarial risk.
The rest of the proof proceeds same as [Sanyal et al., 2021]. We will now
lower bound the probability that there is one mislabeled point placed inside
each lp ball around every point in Î¶.

13

P

^

0

âˆƒs âˆˆ

BÏp (s), (s0 , y)


âˆˆ S for some y s.t. y =
6 c(s )
0

sâˆˆÎ¶

=1âˆ’P

_

0

@s âˆˆ

BÏp (s), (s0 , y)


âˆˆ S for some y s.t. y =
6 c(s )
0

sâˆˆÎ¶

Using the union bound,


0
p
0
0
â‰¥ 1 âˆ’ |Î¶|P for a particular s âˆˆ Î¶, @s âˆˆ BÏ (s), (s , y) âˆˆ S for some y s.t. y 6= c(s )
n

c2
c2
â‰¥ 1 âˆ’ |Î¶|eâˆ’n.Î· |Î¶| â‰¥ 1 âˆ’ Î´
= 1 âˆ’ |Î¶| 1 âˆ’ Î·
|Î¶|

|Î¶|
log |Î¶|
samples suffice for this. This completes the proof.
n â‰¥ Î·c
Î´
2

3.2

Another Theorem

Theorem 2. Let C be a classifier, and D : Rd â†’ Râ‰¥0 the data distribution,
and c : Rd â†’ â„¦ be the ground truth labeling function, where â„¦ is the set of
classes. If there exist constants c1 , c2 , cin , ctotal and R â‰¥ r and a set Î¶ âŠ‚ Rd ,
such that You dont need R for all balls! You can do equally well for smaller
Rs! â‰¤ R is the required condition.


S p
1. cceil â‰¥ Pxâˆ¼D
BR (s) â‰¥ ctotal
sâˆˆÎ¶

2. âˆ€s âˆˆ Î¶, Pxâˆ¼D [Brp (s)] â‰¥

cin
|Î¶|

p
3. âˆ€s âˆˆ Î¶, âˆ€u, v âˆˆ BR
(s), c(u) = c(v)

where BÏp (x) refers to a Ï-radius lp ball around x. Let S be a training set,
obtained by taking n i.i.d. samples from P, and labeling each with c(.) with
Î· probability, or some incorrect class with probability 1 âˆ’ Î·. If a classifier C
is such that
1. C has a 100% training accuracy on S. This means it gets the correct class on unflipped points, and has also memorized the wrong label
on flipped points. Further, on each training point, the classifier C is
confident on the result at least Ï‡ more than the second largest output
probability.
14

2. C is locally lipschitz. We assume that this lipschitzness constant is a
decreasing function Îº of the distance to the nearest point of the opposite
label. So for each point x âˆˆ Rd and x0 in its neighbourhood, with the
nearest point to x with a different label being at a distance greater than
L in lp norm,
JC(x) 6= C(x0 )K
â‰¤ Îº(L)
kx âˆ’ x0 kp
then,


1
R+râˆ’ Îº(Râˆ’r)

P RAdv


(C) â‰¥ ctotal

â‰¥ (1 âˆ’ cceil )

nâˆ’|Î¶|



Î·ncin
e|Î¶|

|Î¶|

p
2Ï€|Î¶|

Proof.

3.3

Crafting Label Noise as a bilevel-optimization

Returning to the problem of crafting label noise, we can frame it as the following optimisation problem. Inspired by recent advances in meta-learning,
there have been successful methods published in many recent works that
approximately solve optimization problems with multiple levels, by backpropagating gradients through each level.
For simplicity, let us assume we have to solve a binary classification problem. There are only 2 classes, and we are given a dataset with n points
sampled from P, and labeled with the ground truth labeling function c to
produce a dataset Sclean = {(xi , c(xi ))}ni=1 .
We are to find one point in this dataset, i.e. some i âˆˆ {1..n}. This point
should be such that if the given pointâ€™s label is flipped to the other class,
the adversarial accuracy of a classifier trained on this modified, or poisoned
dataset is as low as possible.
The following are different agents optimising over different variables in
the problem we are concerned with:
1. The victim optimises over the model parameters, attempting to minimise the training loss
2. The input adversary, as we will call it, optimises over constrained perturbations to the input, attempting to maximise the training loss
15

3. The poisoning adversary, as we will call it, optimises over which points
to carry out a label change, given a budget constraint over the number of labels it can modify, and attempts to minimise the adversarial
accuracy of the model
Note that in each case, the agents are concerned with maximising or minimising accuracy, but the loss is the only metric under control. An appropriate
loss, when minimised or maximised, can be expected to increase/decrease
the accuracy of the model.
Let the size of the training set be n. Let the label noise the poisoning
adversary
adds to the training set be represented as âˆ†y âˆˆ {0, 1}n such that
P
i âˆ†yi = 1, y being the vector of labels in the dataset, ith label being yi ,
X being a matrix storing the training points as its row vectors, with the ith
data point being xi . Let H be the hypothesis class, i.e. the set of functions
the victim is optimising over (one example of a hypothesis class is the set of
all neural networks of a fixed architecture, with different parameter values).
The problem can then be framed as:


Ï
âˆ—
âˆ†y = arg max RAdv arg min L(C, X, y âŠ• âˆ†y)
(3)
CâˆˆH

âˆ†y is one-hot

where âŠ• is the â€xorâ€ operation, i.e. allows label flipping to happen wherever
âˆ†y is one-hot at. This can be practically implemented as


âˆ—
0
âˆ†y = arg max
max
L arg min L(C, X, y âŠ• âˆ†y), X , y
(4)
p
0
âˆ†y is one-hot X âˆˆBÏ (X)

CâˆˆH

where BÏp (X) is defined as the set of all matrices M such that âˆ€i âˆˆ {1..n},
mi âˆˆ BÏp (xi ).
Write about having logits, and using a sigmoid over them. Talk about
the hurdles of using this on practical datasets.

4

Experiments and Results

The first subsection of this Chapter covers many of the initial experiments we
did, and failed at. The rest of the subsections detail the other experiments
that shed light on what happens when networks memorize label noise, and
eventually with a few final experiments showing the nature of the worst points
to mislabel.
16

4.1

Initial Failed experiments

[Sanyal et al., 2021] theoretically gives bounds for the adversarial risk by
reasoning that if mislabeled points are placed at high density regions - regions
where if the classifier memorizes the wrong label, will render a large amount
of probability mass in the vicinity of this mislabeled point vulnerable. This
was the inspiration for the algorithm we studied initially: flipping the labels
of training points which come from high probability density areas.
The algorithm is based on the following idea: For each class of the dataset,
we build graphs over the input space. We consider a hyperparameter Î³, and
build graphs over each class such that there is an edge between every two
points closer than Î³ under some norm q. The degrees of various points
are now a heuristic for the probability density at each point. This kind of a
heuristic seemed to be the starting point, since there is no reliable method yet
that could estimate the probability density at each point, given samples from
a distribution. Variational Autoencoders and the discriminators in GANs in
practice assign high log likelihoods to all points in the training set.
If a point x is mislabeled, then we were expecting all of its neighbours to
be adversarially vulnerable as well. The graphs for each class are individually
analysed, and points are selected to change their labels, using some strategy,
each of which will be detailed below in the forthcoming subsections.
Definition 1. Given a set of points, S = {xi }m
i=1 , the Class Graph of this
set of points is defined as the undirected graph GpÎ³ (S) = (S, EÎ³p (G)), such that
EÎ³p (G) = {{xi , xj } | 0 < i < j â‰¤ m, kxi âˆ’ xj kp â‰¤ Î³}
4.1.1

Building graphs on input space

The first attempt was to build Class Graphs on each class of the MNIST
dataset, and for each class, pick vertices from the graph to flip labels. For
now, we do not concern ourselves with the label we flip to. Each label that
has to be flipped is simply set to the next value cyclically.
After building graphs on the points of each class, we run one of the
following algorithms to select points:
The procedure we followed was as follows:
1. Produce Class Graphs for all classes on the MNIST dataset
2. Select b points to flip using one of the above algorithms
17

Algorithm 2 Greedily Selecting vertices
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

procedure Greedy-Selection(G)
System Initialization
Read the value
if condition = T rue then
Do this
if Condition â‰¥ 1 then
Do that
else if Condition 6= 5 then
Do another
Do that as well
else
Do otherwise
while something 6= 0 do
var1 â† var2
var3 â† var4

. put some comments here
. another comment

Algorithm 3 O(log n) approx to the dominating set problem
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

procedure Greedy-Selection(G)
System Initialization
Read the value
if condition = T rue then
Do this
if Condition â‰¥ 1 then
Do that
else if Condition 6= 5 then
Do another
Do that as well
else
Do otherwise
while something 6= 0 do
var1 â† var2
var3 â† var4
3. Change the labels of the selected points
4. Train networks on the poisoned datasets
18

. put some comments here
. another comment

Figure 2: A 2-d example, showing why the second algorithm might be better
than the first
Figure 3: Figure showing the distribution of the inter-point distances in class
0 of the MNIST dataset. The standard perturbation budget in literature for
MNIST is 0.3 in lâˆ norm, which is very small compared to the distances
observed above.

5. Randomly select b points to flip, and train and evaluate the networks
on them to have a random baseline to compare with
As previously mentioned, the idea behind the above algorithms on the
Class Graphs is that we want to select points that can render as much amount
of probability mass adversarially vulnerable as possible. Since we do not have
a reliable method of density estimation, we select points that have a large
number of neighbouring points, as a heuristic. Flipping a point might render
all of its neighbours vulnerable.
It is a valid question as to why we ran 2 different algorithms: one where
only high degree nodes are selected (greedy strategy), and one which is an
approximate solution to the modified dominating set problem discussed before. When we select only select high degree points, we might be rendering
points from the same neighbourhood vulnerable, which the second algorithm
avoids. Figure 2 shows a visualisation for a set of points, showing why the
dominating set version might be better than the greedy version.
Suprprisingly, the results showed that not only did the above strategies
yield networks that had adversarial errors comparable to the random strategy
(i.e., they are no better), rather, they were strictly inferior. Networks trained
on poisoned datasets modified using the above strategies are more robust
than the ones trained on the same dataset but with random label noise. The
next few subsections illustrate attempts to do the same in different subspaces
which are more feature rich, since it might very well be the case that looking
at the proximity of points in the input representation is sub-optimal for this.
One strong reason why this might be the case is the inter-point distances of
the points in practical datasets we experimented with: MNIST and CIFAR10.
The distances between points is far, far more than the perturbation budget
Ï. Figure 3 illustrates this.
19

4.1.2

Using Low Rank representations

Principal Component Analysis (PCA) is a popular dimensionality reduction method. It aims to find dlow-rank orthogonal directions, given d dimensional data points, such that the dataset has the highest variance along these
dlow-rank directions. The dimensionality reduction is then done by first centering the data to have 0 mean, and then projecting the data points onto each of
the dlow-rank vectors, getting dlow-rank dimensional representations for each of
the points. These dlow-rank directions are obtained by finding out the eigenvectors corresponding to the dlow-rank highest eigenvalues of the covariance
matrix of the data distribution. In practice, the mean and the covariance
matrix are estimated from the given data points. These low-dimensional
representations yielded by PCA are generally referred to as low-rank representations as well.
We calculated the low-rank representations of points in the MNIST training set, and built Class-Graphs on these low-rank representations. This experiment did not yield any conclusions. In certain cases, it performed better
and in certain cases worse, than the random baseline, which is not conclusive
of anything.
4.1.3

Using the feature representations of a trained network

The next attempt was to consider the representations learnt by a trained
network on the dataset. Networks are able to extract rich features, that
heavily correlate with the label. It might be the case that selecting points
that are closer in feature space might perform well. This attempt too, did
not yield anything.
This was rather a bit short sighted, since we later realised, and experimentally verified as well, that networks memorize the false labels by learning
very different feature representations, compared to other points in the vicinity, which have a different (correct) label.
4.1.4

Using the feature representations of a trained network as
well as input space

The final experiment in this direction was to select points that are high-degree
both in the input as well as feature space. We select 2b number of points
from input as well as feature space. The greedy algorithm was employed for
this, since the other one almost selected the same set of points, for the small
20

budgets we had. We then select b points from the intersection of the set of
points selected by both the algorithms.
The above also did not yield definitive results.

4.2

Experiments on a Toy Dataset

After failing in many of the experiments done on CIFAR-10 and MNIST,
we decided to resort to toy distributions to study the problem. We got
interesting results, and the coming sections indicate that poisoning regions
of high probability density is highly sub-optimal for our purpose. There are
3 main factors that determine how large the adversarial error of a model is:
The toy dataset used for the following experiments consists of 2 classes,
with the instance space being Rd . n samples are generated from the distribution P (x) = 12 N (x| âˆ’ Âµ1d , Ïƒ 2 Id ) + 21 N (x|Âµ1d , Ïƒ 2 Id ) with Âµ  Ïƒ 2 , and
labelled with the function c(x) = JxT .1 â‰¥ 0K. This dataset is denoted as
D(d, Âµ, Ïƒ 2 ). From this, we create a poisoned dataset P(d, Âµ, Ïƒ 2 , r) where we
place b mislabeled points for each class, at r distance from the means of the
2 classes, (uniformly sampled at r distance). We have much more control
over experiments in this case, since we know the data distribution, and can
place mislabeled points wherever we want.

Figure 4: Data distribution for d = 2
Figure 4 shows samples from D(d = 2, Âµ, Ïƒ 2 ). We vary Âµ and r, fit networks for each case, and estimate the adversarial accuracy of the fit network
in each case. We carry out these experiments for d = 64, with a 4 layer fully
connected, ReLU activated network. After creating the poisoned dataset,
21

we fit networks with the stated architecture to these poisoned datasets, and
evaluate their adversarial as well as natural accuracies. The results clearly
indicate that placing mislabeled poisons near the mean is the worst for having high adversarial error. There is high variance in the adversarial accuracy
of models given vanilla training, hence we ran multiple seeds for each setup.
For the case where the means were very far apart, the adversarial accuracy
was the highest, and the poisons had almost no effect. Closer the class-means
get, lower the adversarial accuracy.
We also made another change to the setup, and repeated the above experiments. The training setup is the exact same, except that this time, we place
mislabel points to be of a third class, which is not present in the dataset at
all. The network is given one extra unit in its output layer, and all hypterparameters are preserved. All of the trends we observed above were observed
again, but this time the networks had higher adversarial accuracies for each
case.
Another point to note is that while the natural accuracies of the networks
also showed the same trends as the adversarial accuracies, these are on a very
different scale, with all values being upwards the 99% accuracy mark.
We can derive the following conclusions from the above experiments:
1. Adversarial accuracy is hurt more by mislabeling to a class whose decision boundary lies close. This is indicated by the fact that when we
flipped the label to a new, 3rd class, the adversarial accuracy was better
than when we flipped the label to the other class.
2. Placing the mislabeled points at the highest probability density regions
is ineffective when our intention is to increase the adversarial error. In
fact, in many of the above experiment, the networks simply did not
fit the mislabeled points placed at these high density regions. This
is further motivation to not place mislabeled points in such regions,
since we cannot expect the victimsâ€™ models to memorize these points.
They can simply use early stopping, and their models will be as good in
adversarial accuracy as models that would have been trained on clean
datasets.
3. When the class-means are closer to each other, the adversarial error is higher. This indicates that the most adversarially vulnerable
networks simply do not memorize a closed region around mislabeled

22

(a) Adversarial Accuracies

(b) Natural Accuracies

Figure 5: d = 64, b = 2, Ïƒ 2 = 1.0; 2 mislabeled points were placed per class.
points, rather they tweak the decision boundary to include the mislabeled points, rendering large amounts of probability mass vulnerable
â€on the wayâ€. Need to add an illustration showing this. Further experiments add strong evidence in support of this claim.
Till this point, we do not have enough insights to exactly understand
what might be an optimal strategy to flip labels.
23

4.3

Classifiers trained on the poisoned dataset

Mislabeled poisons injected into the dataset are referred to as poisons. Figure
6 shows the results of three experiments (d = 16), which are as follows:
1. A few data points with label 0 were placed near Âµ1d and a few data
points labelled with 1 were placed near âˆ’Âµ1d . Classifiers were then
trained to overfit to this dataset (achieve 100% training accuracy). The
curve shows that adversarial accuracy first decreases and then increases,
as poisons are placed farther and farther away from the means of the
Gaussians.
2. The above experiment was repeated, but the label-flipping is to a third,
new class. This was to see whether the drop in adversarial accuracy
primarily stems from learning more complex decision boundaries in the
first experiment where the poisons were labeled with the opposite class,
or the vulnerability is equally bad when the classifier has to learn new
regions with the third class as the label. The curve obtained in this
case is very similar to the previous experiment, indicating that regions
memorized to be of the opposite class is not worse for the adversarial
accuracy, in comparison to the case where the network is forced to
memorize new regions to be of a completely different, third class.
3. The second experiment is repeated, but with PGD-training, to study
how smoothing out the network on training data also containing poisons affects adversarial accuracy. The curve indicates that there is still
a decline, and eventual increase in the adversarial accuracy of the network, as poisons are placed at greater distances from the means. The
adversarial accuracy was worse in this case compared to the non-PGD
training case (but I am slightly doubtful as to whether this would count
as a fair comparison since PGD training has to be run for a far greater
number of iterations than vanilla training).

4.4

Fitting low dimensional poisoned datasets

This section does not align directly with the problem at hand, and can be
skipped
SGD did not manage to learn networks that get a 100% training accuracy
on the poisoned dataset for the case of 2 dimensional Gaussians when the
24

(a) Poisons have labels set to the
opposite class

(b) Poisons have labels set to a third,
new class

(c) Poisons labelled as a new class, but network
trained with PGD training

Figure 6:
is the test accuracy is the adversarial accuracy. r axis represents the distance from the means of the gaussians where mislabelled points
(poisons) were placed. Setting wrong labels to a third class produces a curve
very similar to the case where labels were set to the opposite class for poisons.
PGD training shows small drop in the natural accuracy, and a huge drop in
the adversarial accuracy. The classifiers achieved 100% training accuracy in
all of the above experiments.
poisons are very close to the means. Instead, the classifier resorts to a simple,
almost linear decision boundary. The following was an experiment to hash
the datapoints using the function in Listing 1 and get SGD to learn complex
decision boundaries when the sense of proximity of correctly labelled points
25

is destroyed. Points in the instance space which are close to each other in
each of the clusters are no longer close after mapping them to a different
space, and the classifier is forced to resort to complex decision boundaries if
a low loss is to be achieved. Figure 7 shows the decision boundaries learned.
SGD managed to hit a 100% training accuracy, and the same training setup
without the hash function was not able to do the same. [Rahaman et al.,
2019] show that SGD learns classifiers that are biased towards first capturing
lower frequency components, and then learning higher frequency ones, even
if the amplitudes of these low frequency components is smaller. Further,
they experiment with a dataset generated using a low frequency function in
1 dimension, and embed it in 2D on increasingly complex manifolds. High
frequency components were learned faster when this manifold has larger complexity, and this might be happening here as well. Passing the data points
through the hash embeds this data in a higher dimensional space, in a complex manifold, where high frequency components are easier to capture.
Listing 1: Function used to map data points âˆˆ Rd to bit sequences that are
shown to the network
def hash ( x ) : # x i s a f l o a t i n g p o i n t number
P = 941
LN = 2Ë†BITS
x = round down ( x âˆ— LN) % P
return b i n a r y r e p r e s e n t a t i o n ( x , l e n g t h=BITS )

4.5

Toy experiment with Lipschitzness constraints on
the network

This experiment is on the toy dataset, with d = 16. We constrain the lipschitzness of the network trained on the poisoned dataset, using spectral
normalization on every layer. The last layerâ€™s spectral norm is varied, and
the results show that the higher this spectral norm, lower the adversarial
accuracy of the network trained on the poisoned datasets. Hence, spectral
normalization as a regularization leads to higher adversarial accuracy. Poisons which were placed very close to the mean of the gaussians, were not
even memorized by networks having a lower spectral norm, which acts as a
natural defence against mislabelled points. Figure 8 shows the results of the
above experiment.

26

Figure 7: Map showing a classifierâ€™s decision boundaries and data points
when the classifier is shown points after passing through the aforementioned
function. The two classes are in red and blue, and green is the third class
that poisons were labelled with.

4.6

The above experiments on MNIST

The attempt here is to see whether we get a curve similar to the one we get
for the toy dataset, which shows that as we place mislabeled points in regions
of increasing probability density, the adversarial accuracy of models trained
on the dataset with these mislabeled points first decreases, and eventually
increases.
We used multiple approaches to estimate the probability density of the
points in the MNIST training set, but none of the experiments yield the curve
we look for.

27

Figure 8: Accuracies of the networks trained with different spectral normalization constraints. Distance of the mislabelled points from the mean is
written inside the sub-figures. For points closer than 2.8, the networks dont
memorize the training set entirely, hence that part has not been shown.
4.6.1

Using degrees as a proxy for the probability density

This experiment was performed on MNIST. To have more control over experiments, we only flip points in class 5, and flip the labels to those of class 6.
We only evaluate the adversarial accuracy of the classifier fit on this poisoned
dataset by carrying out targeted PGD attacks from class 5 to class 6.
28

4.6.2

Fitting gaussians on each class of MNIST

We fit gaussians to each class of MNIST, and score points in the training set
by the density assigned to that point by this fit gaussian.
4.6.3

Fitting gaussians on feature space representations

In this experiment, we first fit a classifier C on the clean MNIST dataset. We
then use the feature representation produced by the penultimate layer of C.
We fit gaussians to this representation, one gaussian for each class. To have
more control over experiments, we only flip points in class 5, and flip the
labels to those of class 6. We only evaluate the adversarial accuracy of the
classifier fit on this poisoned dataset by carrying out targeted PGD attacks
from class 5 to class 6. The curve produced by the above experiment did not
yield any structure, and there was no clear minimum.

4.7

Is it possible that it does not matter which points
we poison?

For high dimensional datasets like MNIST, it might be possible that flipping
certain points does not reduce the adversarial accuracy more than others.
To verify this, we flipped 3 different points from class 5 of MNIST which
were mislabeled to class 6. The resultant networkâ€™s adversarial accuracy was
measured on targeted attacks from class 5 to class 6. This experiment indicates that flipping certain points in the training set is worse for adversarial
accuracy than other points.

4.8

Min Coloring trick

If we could fit one model with each training set point flipped, then we could
iterate over all such models and check which mislabelled points cause the
largest drop in adversarial accuracy. In this experiment, we build graphs
over the training set points of MNIST which belong to class 5, having one
edge between points closer than Î³ in lâˆ distance. We then solve an extension
of the minimum coloring problem on this graph, where even two adjacent
nodes cannot be assigned the same color, and so cant second neighbours. We
only need an approximate solution, which ensures that neighbours or second
neighbours are not assigned the same color. The idea is to fit a network N
29

Figure 9: Only one point from class 5 was flipped to class 6. This was randomly chosen for 3 different runs, and for each run, multiple models were
trained, and their accuracy on targeted attacks from class 5 to 6 was averaged. The figure indicates that some points are worse for adversarial accuracy
compared to others, when mislabeled.
mapping these points to the colors. After N memorizes the colors, we iterate
over every point in class 5, attempting to assign a score to each point. The
score for a point x is determined by the number of its neighbours which were
rendered vulnerable to x, which is estimated by the number of xâ€™s neighbours
such that we can carry out a targeted attack from them, and flip the networkâ€™s
output to the color assigned to x.
The idea behind the above experiment is that the network might be memorizing similar regions of the wrong label for mislabelled points in the poisoned dataset setup as different coloured regions when we assign colors to
the training points. Further, the reason why we ensure that neighbours and
second neighbours are not assigned the same color is that when we start out
with a training point x, with an assigned color c(x), we need to carry out a
targeted attack from this point to the color of one of its neighbours, hence if 2
neighbours share the same color, we cannot be sure which region (pertaining
to which of these neighbours) the attack settled at.
Next, we flip the labels of the training points in class 5 with the largest
scores assigned by the above algorithm. The resulting networkâ€™s adversarial
accuracy is estimated on class 5 only, and that too in a targeted fashion,
30

attempting attacks that flip the networkâ€™s output from 5 to 6, precisely.
The above experiment was compared to the random strategy, where randomly chosen points from class 5 (same fraction of points mislabeled, for
comparison). The random strategy still performed better, compared to points
selected by the above algorithm. This hints towards the idea that there are
points, and a large number of them, such that the network not only learns
a wrong region around them, but probably learns stretched out, far reaching regions from the decision boundary that it would have learnt on a clean
dataset.

4.9
4.9.1

The worst poisons are not the ones producing pockets with the wrong label
Flipping to an 11th class in MNIST

This experiment is on the full MNIST dataset. In this experiment, we selected
a randomly chosen set of training points to flip labels, and we train two
networks. Network A was fit to a dataset in which the labels were flipped to
the next class, and network B to a dataset in which the labels were flipped
to an 11th class (class â€10â€) (this network has 11 logits in its output). Both
networks were trained to fit the poisoned datasets under the same training
setup, and the adversarial accuracy of the networks were estimated.
The network trained with points flipped to the 11th class (network B) had
a much higher adversarial accuracy compared to the other network (network
A). This indicates that the poor adversarial accuracy of models trained with
random label noise is probably because of the mislabeled points stretching
the decision boundary, and rendering large amounts of probability mass vulnerable on the way.
4.9.2

Using the distance from the decision boundary as the score
to select poisons

This experiment was performed on the MNIST dataset, only with classes 0
and 1. We first fit a classifier C, on the clean 0-1-MNIST training set. Next,
we iterate over all points in the training set. For each training point x, we
carry out an unbounded PGD attack from x, till the classifier flips its output.
The number of steps it takes for this unbounded attack to flip the classifier
Câ€™s output, is the value we consider to be the score of the point x.
31

Figure 10:
is the adversarial accuracy of models trained on poisoned
is the adversarial acdatasets with the labels flipped to an 11th class.
curacy of models trained on a poisoned dataset with the poisons mislabeled
to one of the other 9 classes is the adversarial accuracy of models trained
on the clean dataset. The adversarial accuracy when mislabeling is done to
an 11th class is almost as high as the case when there is not label noise.
If there were a reliable way of estimating the probability mass around this
path, computed by the targeted unbounded gradient descent attack, then it
might be optimal to select points for which there is a high probability mass
in the vicinity of the path. This experiment was done with just the path
length.
Figure 11 shows the results of this experiment.

4.10

The worst poisons are the ones stretching the decision boundary

The following section descibes a series of experiments, which reveal why
the random strategy performs surprisingly well in searching for poisons, and
also sheds light on the nature of the points which when mislabeled, cause a
massive drop in the adversarial accuracy of a model trained on the poisoned
dataset.
Two experiments are on binary classification problems, and one on the
full MNIST dataset.

32

Figure 11: 0-1 MNIST: The orange curve (baseline) shows the adversarial
accuracy of the network when mislabeled poisons are randomly selected. The
blue curve shows the adversarial accuracy of different networks trained on
poisoned datasets, where the mislabeled poisons were placed at increasing
distances from the decision boundary (x axis). The x axis (labeled â€binâ€)
is only an ordinal value, and higher values on this axis represent poisoned
datasets where the mislabeled points are at greater distances from the decision boundary that the network would have otherwise learnt on a clean
dataset.
4.10.1

Finding a path from a point to the boundary

The first requirement to the experiments that follow is a method to find a
path from a data point to the decision boundary. For instance, if we are given
a classifier trained on the full MNIST dataset, and are given a data-point
(i.e., some handwritten digit), then we need to find the shortest path from
this datapoint to the decision boundary. We might also have a constraint:
finding a path to the decision boundary of the current data pointâ€™s class, and
a target class. Further, this path is unconstrained, and we are not concerning
ourselves with finding such a path that completely lies inside a constrained
33

(a) Network learning a â€pocketâ€
around a mislabeled point

(b) Network teaking the decision
boundary to â€accommodateâ€ a
mislabaled point

Figure 12: The above figure shows two different ways in which a classifier
could memorize a mislabeled point. If the decision boundary is tweaked to
accommodate a mislabeled point, it could lead to much higher adversarial
errors, compared to the other case.
region. We essentially need to carry out an unbounded gradient descent
attack, and record all the adversarial examples we get on the way.
For a small step size Ï‰, we make gradient ascent updates on the input
image to a model, by backpropagating gradients from the loss, all over to the
inputs. These gradients are used to make small updates over the inputs, but
unlike Projected Gradient Descent, we do not project the input back into an
lp ball of radius Ï around the original input. After every update, we record
the updated input. This terminates only when the output of the classifier is
flipped, or when the process has taken some maximum number of steps. We
will call this procedure untargeted-unbounded-gradient-ascent.
When we have several classes, it makes sense to study the effect of poisons
by evaluating the adversarial accuracy of the model using targeted attacks
from the original class of the poisons, to the class the poisons were mislabeled to. For instance, if some points in class 9 of the MNIST dataset were
mislabeled to class 0, then it makes sense to evaluate the robustness of the

34

resultant model using targeted adversarial attacks from class 9 to class 0.
We hence, find out paths in multi-class settings by recording the path from
a given data point to a target decision boundary. We will call this procedure
targeted-unbounded-gradient-ascent.
We will refer to the paths traced out by the above algorithms as adversarial paths.
Interestingly, when we found out the adversarial paths for some of the
below experiments, we came across label noise already present in MNIST!
We simply selected points closest to the decision boundary, and plotted them
out. Many of the points close to the decision boundary already have label
noise.
4.10.2

Using adversarial paths to analyse poisons

For a given dataset, we first train a classifier on the clean datasets, without
placing any mislabeled poisons. We then iterate over the training set, and
find out adversarial paths for each point.
The hypothesis is that if points are too close to the decision boundary,
then it is not very optimal to mislabel them, since the decision boundary
is going to slightly be modified to accommodate the poison. If it is too far
away, then it might be the case that having a lot of correctly labeled points
after that forces the classifier to memorize the poison using an alternative
pocket, rather than modify the decision boundary it would have otherwise
learnt.
This is illustrated by the figure ...
PGD paths of networks trained on MNIST
We consider the case where we flip selected pointsâ€™ labels to the next class
cyclically. The architecture of the network is the standard one.
4.10.3

The experiments

1. 0-1 MNIST
We only consider a 2-class dataset, composed of the classes â€0â€ and
â€1â€, from MNIST.
A binary classifier is trained on this dataset. We then find out the
adversarial paths for each point in the training set. Figure 14 shows
an illustration of the distribution of the length of these paths over the
training set points, per class.
35

(a) test

(b)

Figure 13: After finding out the adversarial paths from each training point
to the decision boundary of a model trained on MNIST, we visualized some
points closest to and farthest from the decision boundary. (a)
shows the 5 points from each class closest to the decision boundary, and (b)
shows 5 points farthest from the boundary for each class. Examples
pertaining to each class have been put on different rows: 0 to 9 from top to
bottom.

36

Next, we divide the training set points into various disjoint buckets,
each bucket holding points from a different range of the length of the
adversarial path. Essentially, the x-axis of 14 is divided into ranges,
and points falling into each range are put into the bucket for that range.

Figure 14: Distribution over the number of gradient ascent updates it took
for the classifier to flip its output, for 0-1 MNIST, for the points in the
training set. The y-axis indicates the number of points in the training set.
The step size used for the above was 0.01, on [0, 1] normalized input images.
We train several models for each bucket, each with randomly selected
poisons from that bucket. Figure 15 illustrates the results. Closer
the mislabeled points are to the original decision boundary, less the
fall in the adversarial accuracy of the model trained after mislabeling
that point. As we go farther from the decision boundary, the poisons
have a greater impact. The next experiment will shed more light on
placing poisons even farther from the boundary (in the full MNIST
case, where some classes were observed to be even farther from the
decision boundary), and the impact of the poisons starts wearing away.
2. MNIST We now consider the full MNIST dataset, and any points select to be mislabeled, will be cyclically set to the next class in [0..9].
We first, like done previously, find out the adversarial paths for each
training set point. But since we are flipping the label to the next class,
we will find out adversarial paths for each point in a targeted manner,
with the next class being the target.
37

Figure 15: The x axis only denotes an ordinal value: higher values represents represents buckets holding points for which the adversarial paths were
lengthier.
16 shows the distribution of the number of gradient descent steps it
took, i.e. the length of the targeted-adversarial paths for each point
in the training set. First of all, we observe that we cannot have universal bucket for all classes, since some classes are much farther from
the decision boundary than others. So we will have empty buckets for
such ranges, very far off from the decision boundary. There is a solution however, which is that we only place mislabeled poisons for one
class of the dataset, flipping the label cyclically to the next class. The
adversarial accuracy of the resultant network will only be evaluated on
targeted attacks from that selected class, to the next one (the class the
labels were flipped to).
The results of this experiment show that the closer the mislabeled
points are to the decision boundary, lower their impact. As we mislabel points farther from the boundary, the impact increases, until a
certain point. After a point, selecting points farther from the decision
boundary worsens the impact.

38

Also, the minimum observed here and for the 0-1 case is where most
of the training set points lie, which explains why the random strategy
manages to cause a huge fall in the adversarial accuracy. Randomly
selecting points to flip still has a high chance of selecting these impactful
points, since there are so many of them.

Figure 16: Distribution over the number of gradient ascent updates it took
for the classifier to flip its output, for the full MNIST dataset, for the points
in the training set. The y-axis indicates the number of points in the training
set. The step size used for the above was 0.01, on [0, 1] normalized input
images. Note that for class â€1â€, it takes not more than 10 steps for almos
the entire training set, which means for an attack radius of  = 0.1, the entire
class must be vulnerable, which is indeed the case. The adversarial accuracy
of the model on targeted attacks from 1 to 2 is 0.
3. Toy Dataset

39

Figure 17: Full MNIST dataset. The x axis only denotes an ordinal value:
higher values represents represents buckets holding points for which the adversarial paths were lengthier. The above graphâ€™s adversarial evaluation is
only on class â€9â€, being attacked in a targeted manner to class â€0â€. The orange line is a baseline showing the adversarial accuracy when same number
of points are randomly selected to poison. 24 runs were taken for the random
baseline.

40

References
Peter L Bartlett, Philip M Long, GaÌbor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. Proceedings of the National Academy
of Sciences, 2020.
Mikhail Belkin, Siyuan Ma, and Soumik Mandal.
To understand
deep learning we need to understand kernel learning. arXiv preprint
arXiv:1802.01396, 2018.
I. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. CoRR, abs/1412.6572, 2015.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359â€“
366, 1989.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras,
and Adrian Vladu. Towards deep learning models resistant to adversarial
attacks. In 6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference
Track Proceedings. OpenReview.net, 2018. URL https://openreview.
net/forum?id=rJzIBfZAb.
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin,
Fred A. Hamprecht, Yoshua Bengio, and Aaron Courville. On the spectral
bias of neural networks, 2019.
Amartya Sanyal, Varun Kanade, Puneet K. Dokania, and Philip H.S. Torr.
How benign is benign overfitting ? In Submitted to International Conference on Learning Representations, 2021. URL https://openreview.net/
forum?id=g-wu9TMPODo. under review.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru
Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural
networks. In International Conference on Learning Representations, 2014.
URL http://arxiv.org/abs/1312.6199.

41

